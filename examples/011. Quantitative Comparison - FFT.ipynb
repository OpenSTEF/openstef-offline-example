{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "54d26f6b61632442",
   "metadata": {},
   "source": [
    "# Comparison on difficult cases\n",
    "\n",
    "With this notebook, you can compare how changes to OpenSTEF improve it's performance on concrete cases where the performance of OpenSTEF was not that great. \n",
    "\n",
    "At Alliander, the operations team identified a number of cases where improvements of the forecast accuracy would be very valuable.\n",
    "These cases are stored as 'fixed' train/test data experiments that can be used to systematically measure improvement ideas to OpenSTEF.\n",
    "\n",
    "This notebook uses the `train_pipeline_common` function from OpenSTEF, which allows detailed modifications of the input data.\n",
    "\n",
    "The notebook is structured as follows:\n",
    "- Generate benchmark (needed once)\n",
    "  - For each experiment:\n",
    "    - read data\n",
    "    - train model\n",
    "    - generate forecast\n",
    "  - Calculate performance metrics\n",
    "  - (Optionally: repeat N times to make outcome more robust)\n",
    "- Test improvement idea (import local OpenSTEF)\n",
    "  - For each experiment:\n",
    "    - read data\n",
    "    - train model\n",
    "    - generate forecast\n",
    "  - Calculate performance metrics\n",
    "  - (Optionally: repeat N times to make outcome more robust)\n",
    "- Compare benchmark and improvement idea"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import timedelta\n",
    "from tqdm.notebook import tqdm\n",
    "import plotly.express as px\n",
    "from typing import Dict, List\n",
    "from sklearn import metrics\n",
    "import plotly.subplots as subplots\n",
    "import scipy.signal as signal\n",
    "from sklearn import metrics\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "# Set plotly as the default pandas plotting backend\n",
    "pd.options.plotting.backend = \"plotly\"\n",
    "import plotly.io as pio\n",
    "\n",
    "pio.renderers.default = \"plotly_mimetype+notebook\"\n",
    "\n",
    "# Import required stuff from OpenSTEF\n",
    "from openstef.data_classes.prediction_job import PredictionJobDataClass\n",
    "from openstef.data_classes.model_specifications import ModelSpecificationDataClass\n",
    "\n",
    "from openstef.metrics.figure import plot_feature_importance\n",
    "from openstef.pipeline.train_model import train_model_pipeline\n",
    "from openstef.pipeline.create_forecast import create_forecast_pipeline\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import logging\n",
    "logging.root.setLevel(logging.INFO)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdda40a442732577",
   "metadata": {},
   "source": [
    "## Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed6b112cd38c8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "difficult_location_number = 1  # 1, 2 of 3\n",
    "quantiles = [0.05, 0.1, 0.3, 0.5, 0.7, 0.9, 0.95]\n",
    "\n",
    "PLOT_POSITIVE_PEAKS = True\n",
    "PLOT_NEGATIVE_PEAKS = False\n",
    "compare_positive_peaks = True  # For quantile peak detection evaluation\n",
    "\n",
    "PLOT_ROLLING_WINDOW = False\n",
    "\n",
    "percentile = 50  # Select forecast percentile to evaluate\n",
    "\n",
    "# Less important settings:\n",
    "\n",
    "# For rolling window calculations. 1 sample = window of 15 minutes\n",
    "window_size = 4 * 24\n",
    "\n",
    "# Peak detection settings\n",
    "peak_evaluation_width = 10\n",
    "peak_miss_threshold = 1e6\n",
    "load_threshold_min_quantile = 0.1\n",
    "load_threshold_max_quantile = 0.9\n",
    "peak_distance = 4 * 3  # 3 hours\n",
    "\n",
    "\n",
    "repeat_count = 5 # Repeats experiment to make outcome more robust"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9562fef",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "import scipy.fft as sf\n",
    "import numpy as np\n",
    "\n",
    "fft_window_size = int(7 * 24 * 60 / 15) # 7 days\n",
    "fft_forecast_N = 192 # 2 days\n",
    "fft_horizons = [0, 2, 4, 48, 96, 192]\n",
    "with_fft = False\n",
    "\n",
    "def fourier_forecast(\n",
    "        signal: np.ndarray,\n",
    "        top_K: int = 5,\n",
    "        forecast_N: int = 192,\n",
    "):\n",
    "    N = signal.shape[0]\n",
    "    X = sf.rfft(signal) / N\n",
    "    X_thresholded = np.copy(X)\n",
    "    top_indices = np.argsort(abs(X_thresholded))[::-1][top_K:]\n",
    "    X_thresholded[top_indices] = 0\n",
    "    \n",
    "    # Generate restored signal\n",
    "    frequencies = np.fft.rfftfreq(N, d=1.0)\n",
    "    t = np.arange(0, N + forecast_N)\n",
    "    signal_restored = np.zeros(t.size)\n",
    "    for i in range(len(X_thresholded)):\n",
    "        amplitude = np.absolute(X_thresholded[i]) / N\n",
    "        phase = np.angle(X_thresholded[i])\n",
    "        signal_restored += amplitude * np.cos(2 * np.pi * frequencies[i] * t + phase)\n",
    "    \n",
    "    # Normalize the signal\n",
    "    signal_restored = (signal_restored / abs(signal_restored).max()) * signal.max()\n",
    "    \n",
    "    return signal_restored\n",
    "\n",
    "# The following function adds a fourier forecast feature to the dataset\n",
    "# It takes an arbitrary long signal, a window size and forecast horizons\n",
    "# For each point in the signal, it calculates the fourier forecast using the previous window\n",
    "\n",
    "def fourier_forecast_feature(\n",
    "        signal: np.ndarray,\n",
    "        window_size: int,\n",
    "        horizons: List[int],\n",
    "        top_K: int = 5,\n",
    "):\n",
    "    N = signal.shape[0]\n",
    "    forecast_features = np.zeros((N, len(horizons)))\n",
    "    forecast_features[:, :] = np.nan\n",
    "    \n",
    "    for i in range(N):\n",
    "        if i < window_size:\n",
    "            continue\n",
    "            \n",
    "#         signal_window = signal[i - window_size+1:i+1] # +1 to include the current point\n",
    "        signal_window = signal[i - window_size:i] # +1 to include the current point\n",
    "        forecast = fourier_forecast(signal_window, top_K=top_K, forecast_N=max(horizons) + 1)[window_size:]\n",
    "        \n",
    "        for j, horizon in enumerate(horizons):\n",
    "            forecast_features[i, j] = forecast[horizon]\n",
    "            \n",
    "    return forecast_features\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "684670301a3d7d33",
   "metadata": {},
   "source": [
    "## Generate Benchmark\n",
    "Generate a benchmark using the latest OpenSTEF. Only needed once."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a31e78486be6833",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load experiment\n",
    "def get_experiments(difficult_location_number: int = 1, n_days: int = 10):\n",
    "    \"\"\"\n",
    "    Returns a list of dicts, with each dict being an experiment with:\n",
    "    - name\n",
    "    - dataset\n",
    "    - test_length # length counted from the end of the data that will be considered the Test period\n",
    "    -\"\"\"\n",
    "    complete_data = pd.read_csv(\n",
    "        f\"data/model_input_difficult_location_{difficult_location_number}.csv\",\n",
    "        parse_dates=True,\n",
    "        index_col=0,\n",
    "    )[:-3]\n",
    "\n",
    "    def create_dataset(i: int):\n",
    "        dataset = complete_data.iloc[0 * 24 * 4: (120 + i) * 24 * 4 + 24 * 4]\n",
    "        dataset = dataset[~dataset[\"load\"].isna()]\n",
    "        \n",
    "        if with_fft:\n",
    "            fft_features = fourier_forecast_feature(\n",
    "                dataset[\"load\"].values,\n",
    "                window_size=fft_window_size,\n",
    "                horizons=fft_horizons,\n",
    "                top_K=10,\n",
    "            )\n",
    "            for horizon in fft_horizons:\n",
    "                dataset[f\"fft_{horizon}\"] = fft_features[:, fft_horizons.index(horizon)]\n",
    "        \n",
    "        dataset = dataset.iloc[fft_window_size:]\n",
    "        \n",
    "        return dataset\n",
    "\n",
    "    # Generate the experiments where we forecast for 1 day each time\n",
    "    # Dataset contains both the 120 days of training data + the 1 day of test data\n",
    "    result = [\n",
    "        dict(  # Predict day 1\n",
    "            name=f\"N{i + 1}_loc{difficult_location_number}\",\n",
    "            dataset=create_dataset(i),\n",
    "            test_length=24 * 4 if i != n_days - 1 else 24 * 4 - 3,\n",
    "        )\n",
    "        for i in range(10)\n",
    "    ]\n",
    "    return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "771bb027559d4099",
   "metadata": {},
   "outputs": [],
   "source": [
    "experiments = get_experiments(difficult_location_number)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28ee318adfba09e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shared prediction job specs\n",
    "pj = dict(\n",
    "    id=1,  # Should be updated for each experiment\n",
    "    model=\"xgb\",\n",
    "    quantiles=quantiles,\n",
    "    name=\"benchmark\",\n",
    ")\n",
    "\n",
    "# These are not relevant...\n",
    "pj_specs_that_should_be_optional_in_future_openstef_versions = dict(\n",
    "    forecast_type=\"demand\",\n",
    "    lat=52.0,\n",
    "    lon=5.0,\n",
    "    horizon_minutes=47 * 60,\n",
    "    description=\"description\",\n",
    "    resolution_minutes=15,\n",
    "    hyper_params={\n",
    "        \n",
    "    },\n",
    "    feature_names=None,\n",
    ")\n",
    "\n",
    "pj.update(pj_specs_that_should_be_optional_in_future_openstef_versions)\n",
    "\n",
    "pj = PredictionJobDataClass(\n",
    "    **pj,\n",
    "#     default_modelspecs=ModelSpecificationDataClass(\n",
    "#         id=1,\n",
    "#         hyper_params={'learning_rate': 0.17808703256781336, 'alpha': 0.323528993939568, 'lambda': 0.7770440758449313, 'subsample': 0.8530034572050896, 'min_child_weight': 14, 'max_depth': 10, 'colsample_bytree': 0.8846288357055685, 'max_delta_step': 0, 'gamma': 0.3223431809617294}\n",
    "#     )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c96767cc1245df4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "forecasts = pd.DataFrame()\n",
    "experiment_forecasts_lists = []\n",
    "\n",
    "for i, experiment in tqdm(enumerate(experiments)):\n",
    "    forecast_list = []\n",
    "    for c in range(repeat_count):\n",
    "        print(f\"Starting experiment: {experiment['name']}\")\n",
    "        #############\n",
    "        # Data prep\n",
    "        # Update pj\n",
    "        pj[\"id\"] = i + random.randint(0, 1000000)\n",
    "        pj[\"description\"] = experiment[\"name\"]\n",
    "        if pj.default_modelspecs:\n",
    "            pj[\"default_modelspecs\"][\"id\"] = pj[\"id\"]\n",
    "\n",
    "        # Split train and test\n",
    "        train = experiment[\"dataset\"].iloc[: -experiment[\"test_length\"]]\n",
    "        realised = experiment[\"dataset\"].iloc[-experiment[\"test_length\"]:][\"load\"]\n",
    "\n",
    "        test = experiment[\"dataset\"].copy(deep=True)\n",
    "        # Set the test data to NaN for the last test_length rows\n",
    "        test.iloc[-experiment[\"test_length\"]:, 0] = (\n",
    "            np.nan\n",
    "        )  # This assumes the load column is the first one!\n",
    "        # For forecasting we use the last 14 days of data as historical data, which is used for the lagged features\n",
    "        # So we select the last 14 days of the training data + the day to forecast\n",
    "        test = test.iloc[-15 * 24 * 4:]\n",
    "\n",
    "        ##############\n",
    "        # train model\n",
    "        models = train_model_pipeline(\n",
    "            pj,\n",
    "            train,\n",
    "            check_old_model_age=False,\n",
    "            mlflow_tracking_uri=\"./mlflow_trained_models\",\n",
    "            artifact_folder=\"./mlflow_artifacts\",\n",
    "        )\n",
    "\n",
    "        #################\n",
    "        # Generate forecast\n",
    "        forecast = create_forecast_pipeline(\n",
    "            pj,\n",
    "            test,\n",
    "            mlflow_tracking_uri=\"./mlflow_trained_models\",\n",
    "        )\n",
    "        # Add realised to forecast\n",
    "        forecast[\"load\"] = realised\n",
    "        # Only keep region that was actually forecasted\n",
    "        forecast = forecast.iloc[-experiment[\"test_length\"]:]\n",
    "\n",
    "        #######\n",
    "        # Store forecast / Concatenate results\n",
    "        _forecasts = pd.concat([forecasts, forecast], axis=0)\n",
    "        forecast_list.append(_forecasts)\n",
    "    \n",
    "    experiment_forecasts_lists.append(forecast_list)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7ab092ae6454ab",
   "metadata": {},
   "source": [
    "## Evaluate results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4542551722baa44a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get all column names that start with \"quantile\"\n",
    "quantile_columns = [col for col in experiment_forecasts_lists[0][0].columns if col.startswith(\"quantile\")]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a301aae4a5d59c2c",
   "metadata": {},
   "source": [
    "### rMAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c62a86fb63c0eaf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment_rmae_lists = []\n",
    "\n",
    "for forecasts_list in experiment_forecasts_lists:\n",
    "    rmae_list = []\n",
    "\n",
    "    # calculate rmae between forecast and measurements\n",
    "    for fc in forecasts_list:\n",
    "        forecast = fc[f\"quantile_P{percentile}\"]\n",
    "        measurements = fc[\"load\"]\n",
    "\n",
    "        rmae = np.sqrt(metrics.mean_squared_error(measurements, forecast))\n",
    "        rmae_list.append(rmae)\n",
    "\n",
    "    print(f'Mean rMAE: {np.mean(rmae_list)}')\n",
    "    print(f'Std rMAE: {np.std(rmae_list)}')\n",
    "    \n",
    "    experiment_rmae_lists.append(rmae_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af049431385914c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_peaks_from_measurements(\n",
    "        measurements,\n",
    "        thresholds_pos: [int],\n",
    "        thresholds_neg: [int],\n",
    "        positive_peaks: bool = True,\n",
    "        peak_distance: int = 12,\n",
    ") -> [int]:\n",
    "    \"\"\"\n",
    "\n",
    "    Args:\n",
    "        measurements (_type_): _description_\n",
    "        thresholds_pos (int]): _description_\n",
    "        thresholds_neg (int]): _description_\n",
    "        positive_peaks (bool, optional): _description_. Defaults to True.\n",
    "        peak_distance (int, optional): Sample distance between peaks. Defaults to 12 (3 * 4 samples = 3 hours).\n",
    "\n",
    "    Returns:\n",
    "        _type_: _description_\n",
    "    \"\"\"\n",
    "\n",
    "    # measurements = measurements.droplevel([1], axis=1)\n",
    "\n",
    "    # Define constants\n",
    "\n",
    "    peak_distance = 4 * 3  # samples are 15min, 3 hour distance\n",
    "\n",
    "    # Detect peaks\n",
    "    if positive_peaks:\n",
    "        peaks = [\n",
    "            (\n",
    "                \"terminal_id\",\n",
    "                signal.find_peaks(\n",
    "                    measurements, height=thresholds_pos, distance=peak_distance\n",
    "                ),\n",
    "            )\n",
    "        ]\n",
    "    else:\n",
    "        peaks = [\n",
    "            (\n",
    "                \"terminal_id\",\n",
    "                signal.find_peaks(\n",
    "                    -measurements, height=-thresholds_neg, distance=peak_distance\n",
    "                ),\n",
    "            )\n",
    "        ]\n",
    "\n",
    "    return peaks\n",
    "\n",
    "\n",
    "def is_missed_peak_in_window(\n",
    "        peak_measurement_window, peak_forecast_window, miss_threshold: int\n",
    "):\n",
    "    mape = metrics.mean_absolute_error(peak_measurement_window, peak_forecast_window)\n",
    "    return mape > miss_threshold\n",
    "\n",
    "\n",
    "def is_missed_peak(\n",
    "        comp_df, peak_index: int, peak_evaluation_width: int, mape_threshold: int\n",
    "):\n",
    "    min_index = max(peak_index - peak_evaluation_width, 0)\n",
    "    max_index = min(peak_index + peak_evaluation_width, len(comp_df))\n",
    "    measurement_window = comp_df.loc[:, \"Measured\"][min_index:max_index]\n",
    "    forecast_window = comp_df.loc[:, \"Forecast\"][measurement_window.index]\n",
    "    return is_missed_peak_in_window(measurement_window, forecast_window, mape_threshold)\n",
    "\n",
    "\n",
    "def get_missed_peaks(\n",
    "        comp_df, peak_index_list: [int], peak_evaluation_width: int, mape_threshold: int\n",
    "):\n",
    "    return [\n",
    "        peak\n",
    "        for peak in peak_index_list\n",
    "        if is_missed_peak(comp_df, peak, peak_evaluation_width, mape_threshold)\n",
    "    ]\n",
    "\n",
    "\n",
    "def count_missed_peaks(\n",
    "        comp_df,\n",
    "        peak_index_list: [int],\n",
    "        peak_evaluation_width: int,\n",
    "        mape_threshold: int,\n",
    "):\n",
    "    total_missed = 0\n",
    "    for peak in peak_index_list:\n",
    "        min_index = max(peak - peak_evaluation_width, 0)\n",
    "        max_index = min(peak + peak_evaluation_width, len(comp_df))\n",
    "        measurement_window = comp_df.loc[:, \"Measured\"][min_index:max_index]\n",
    "        forecast_window = comp_df.loc[:, \"Forecast\"][measurement_window.index]\n",
    "\n",
    "        # Plot red area around missed peak\n",
    "        if is_missed_peak_in_window(\n",
    "                measurement_window, forecast_window, mape_threshold\n",
    "        ):\n",
    "            total_missed += 1\n",
    "\n",
    "    return total_missed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac63433c6a9fc946",
   "metadata": {},
   "outputs": [],
   "source": [
    "forecast_percentile = percentile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16f369b51d74989",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "experiment_total_missed_pos_lists = []\n",
    "peaks_neg_list = []\n",
    "\n",
    "for forecasts_list in experiment_forecasts_lists:\n",
    "    for fc in forecasts_list:\n",
    "        measurements = fc[\"load\"]\n",
    "        total_missed_pos_list = []\n",
    "\n",
    "        thresholds_pos = measurements.quantile(load_threshold_max_quantile)\n",
    "        thresholds_neg = measurements.quantile(load_threshold_min_quantile)\n",
    "\n",
    "        peaks_pos = get_peaks_from_measurements(\n",
    "            measurements,\n",
    "            thresholds_pos=thresholds_pos,\n",
    "            thresholds_neg=thresholds_neg,\n",
    "            positive_peaks=True,\n",
    "            peak_distance=peak_distance,\n",
    "        )\n",
    "        peaks_neg = get_peaks_from_measurements(\n",
    "            measurements,\n",
    "            thresholds_pos=thresholds_pos,\n",
    "            thresholds_neg=thresholds_neg,\n",
    "            positive_peaks=False,\n",
    "            peak_distance=peak_distance,\n",
    "        )\n",
    "\n",
    "        for i, peak_list in enumerate(peaks_pos):\n",
    "            terminal_id, peaks = peak_list\n",
    "            measurements_for_terminal = measurements.rename(\"Measured\")\n",
    "            forecasts_for_terminal = fc[f\"quantile_P{forecast_percentile}\"].rename(\n",
    "                \"Forecast\"\n",
    "            )\n",
    "\n",
    "            comp_df = pd.concat([measurements_for_terminal, forecasts_for_terminal], axis=1)\n",
    "            print(comp_df.isnull().values.any())\n",
    "            total_missed_pos = count_missed_peaks(\n",
    "                comp_df, peaks[0], peak_evaluation_width, peak_miss_threshold\n",
    "            )\n",
    "\n",
    "            print(f\"Missed positive peaks: {total_missed_pos}\")\n",
    "            total_missed_pos_list.append(total_missed_pos)\n",
    "\n",
    "        experiment_total_missed_pos_lists.append(total_missed_pos_list)\n",
    "    \n",
    "    print(f'Avg missed pos peaks: {np.mean(total_missed_pos_list)}')\n",
    "    print(f'Std missed pos peaks: {np.std(total_missed_pos_list)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1628fce8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import datetime as dt\n",
    "import os\n",
    "\n",
    "result_data = []\n",
    "\n",
    "for i in range(len(experiment_forecasts_lists)):\n",
    "    print(f'\\n\\n===== Experiment {i} =====')\n",
    "    print(f'Mean rMAE:', f'{np.mean(experiment_rmae_lists[i]):.2f}'.rjust(15, ' '))\n",
    "    print(f'Std rMAE :', f'{np.std(experiment_rmae_lists[i]):.2f}'.rjust(15, ' '))\n",
    "    print(f'Avg missed pos peaks: {np.mean(experiment_total_missed_pos_lists[i]):.1f}')\n",
    "    print(f'Std missed pos peaks: {np.std(experiment_total_missed_pos_lists[i]):.1f}')\n",
    "    \n",
    "    result_data.append({\n",
    "        'experiment': i,\n",
    "        'mean_rmae': np.mean(experiment_rmae_lists[i]),\n",
    "        'std_rmae': np.std(experiment_rmae_lists[i]),\n",
    "        'avg_missed_pos_peaks': np.mean(experiment_total_missed_pos_lists[i]),\n",
    "        'std_missed_pos_peaks': np.std(experiment_total_missed_pos_lists[i]),\n",
    "    })\n",
    "    \n",
    "os.makedirs('experiment_results', exist_ok=True)\n",
    "result_name = f\"experiment_results/difficult_location_{difficult_location_number}_results_fft={with_fft}_{dt.datetime.now().strftime('%Y-%m-%d_%H:%M:%S')}.json\"\n",
    "\n",
    "with open(result_name, 'w') as f:\n",
    "    json.dump(result_data, f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

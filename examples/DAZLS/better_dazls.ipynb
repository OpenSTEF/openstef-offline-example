{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bd659c77",
   "metadata": {},
   "source": [
    "# DAZLS MODEL\n",
    "## Domain Adaptation for Zero-Shot Learning in Sequence"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f628b90",
   "metadata": {},
   "source": [
    "**DAZLS** is the new energy splitting method which is going to be used in openSTEF.\n",
    "\n",
    "It trains one splitting model which can be used for every prediction job.\n",
    "\n",
    "As input it uses data from multiple substations with known components and it outputs the prediction of solar and wind power for unkown target substations.\n",
    "\n",
    "The model contains 3-steps which are deployed in sequence:\n",
    "1. Domain model (any data-driven model can be used)\n",
    "2. Adaptation model (any data-driven model can be used)\n",
    "3. Physical Correction model (it is specific to this task, but can be adapted)\n",
    "\n",
    "For validation, the approach Leave-one-out is being used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c200fa1b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HAL\n",
      "RMSE test= 0.03685411732317427\n",
      "R-sqr= 0.30408593867613676\n",
      "HFDP\n",
      "RMSE test= 0.29731055878988316\n",
      "R-sqr= -1.3908466829338044\n",
      "LLS\n",
      "RMSE test= 0.18550646112011965\n",
      "R-sqr= 0.6928684901176348\n",
      "MNZL\n",
      "RMSE test= 0.06787005934175006\n",
      "R-sqr= 0.2971844944391836\n",
      "NRYN\n",
      "RMSE test= 0.060716139554765194\n",
      "R-sqr= 0.692598476068661\n",
      "OWD\n",
      "RMSE test= 0.0983913348736813\n",
      "R-sqr= 0.8999194024661905\n",
      "WEW\n",
      "RMSE test= 0.0879411676678399\n",
      "R-sqr= 0.5557680473477729\n",
      "WHF\n",
      "RMSE test= 0.031980482086807246\n",
      "R-sqr= 0.8101134986300711\n",
      "WLS\n",
      "RMSE test= 0.04693656013938843\n",
      "R-sqr= 0.7355097726581723\n",
      "WWF\n",
      "RMSE test= 0.03471334262302751\n",
      "R-sqr= 0.7062994479576562\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error\n",
    "import glob\n",
    "import pandas as pd\n",
    "import os\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.decomposition import FastICA\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler, MaxAbsScaler, RobustScaler, Normalizer, normalize\n",
    "from sklearn.linear_model import Lasso, MultiTaskLassoCV\n",
    "from sklearn.ensemble import RandomForestRegressor, AdaBoostRegressor\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.gaussian_process import GaussianProcessRegressor\n",
    "from sklearn.decomposition import KernelPCA, PCA\n",
    "import random\n",
    "from sklearn.utils import shuffle\n",
    "from scipy.linalg import fractional_matrix_power\n",
    "from sklearn.covariance import LedoitWolf, OAS\n",
    "from sklearn.base import BaseEstimator\n",
    "\n",
    "# Seed, path, etc,\n",
    "random.seed(999)\n",
    "np.random.seed(999)\n",
    "\n",
    "path = os.path.dirname(os.path.abspath(\"prep_data\"))\n",
    "folder = ['\\\\prep_data\\\\']\n",
    "combined_data = []\n",
    "station_name = []\n",
    "\n",
    "# Read prepared data\n",
    "for file_name in glob.glob(path + folder[0] + '*.csv'):\n",
    "    x = pd.read_csv(file_name, low_memory=False, parse_dates=[\"datetime\"])\n",
    "    x[\"datetime\"] = pd.to_datetime(x[\"datetime\"])\n",
    "    x = x.set_index('datetime')\n",
    "    x.columns=[x.lower() for x in x.columns]\n",
    "    combined_data.append(x)\n",
    "    sn = os.path.basename(file_name)\n",
    "    station_name.append(sn[:len(sn) - 4])\n",
    "\n",
    "\n",
    "# DAZLS algorithm\n",
    "class DAZLS(BaseEstimator):\n",
    "    def __init__(self):\n",
    "        self.__name__ = \"DAZLS\"\n",
    "        self.xscaler = None\n",
    "        self.x2scaler = None\n",
    "        self.yscaler = None\n",
    "        self.domain_model = None\n",
    "        self.adaptation_model = None\n",
    "        self.mini = None\n",
    "        self.maxi = None\n",
    "        self.on_off = None\n",
    "        self.target_substation_pred_data = None\n",
    "        self.y_test = None\n",
    "\n",
    "    def fit(self, data, xindex, x2index, yindex, n, domain_model_clf, adaptation_model_clf, n_delay, cc):\n",
    "        x_index = list(set(np.arange(0, nn)) - set([n]))\n",
    "        y_index = n\n",
    "        on_off = np.asarray(combined_data[n].iloc[:, [n_delay * 3 + 4, n_delay * 3 + 5]])\n",
    "        ###### GPS DIFFERENCE#######\n",
    "        diff_index1 = [n_delay * 3 + 2, n_delay * 3 + 3]  # GPS location\n",
    "        diff_index2 = list(np.arange(n_delay * 3 + 8, cc))  # Variance and SEM\n",
    "\n",
    "        for nx in range(nn):\n",
    "            for ff in diff_index1:\n",
    "                combined_data[nx].iloc[:, ff] = (combined_data[nx].iloc[:, ff] - combined_data[n].iloc[:, ff])\n",
    "            for fff in diff_index2:\n",
    "                combined_data[nx].iloc[:, fff] = (combined_data[nx].iloc[:, fff] - combined_data[n].iloc[:, fff])\n",
    "\n",
    "        ####################  CALIBRATION #################################\n",
    "        temp_data = [combined_data[ind] for ind in x_index]  # Without the target substation\n",
    "        ori_data = np.concatenate(temp_data, axis=0)\n",
    "        test_data = np.asarray(combined_data[y_index])\n",
    "        X, X2, y = ori_data[:, xindex], ori_data[:, x2index], ori_data[:, yindex]\n",
    "        domain_model_input, adaptation_model_input, y_train = shuffle(X, X2, y, random_state=999)  # just shuffling\n",
    "        domain_model_test_data, adaptation_model_test_data, y_test = test_data[:, xindex], test_data[:, x2index], test_data[:, yindex]\n",
    "        xscaler = MinMaxScaler(clip=True)\n",
    "        x2scaler = MinMaxScaler(clip=True)\n",
    "        yscaler = MinMaxScaler(clip=True)\n",
    "        X_scaler = xscaler.fit(domain_model_input)\n",
    "        X2_scaler = x2scaler.fit(adaptation_model_input)\n",
    "        y_scaler = yscaler.fit(y_train)\n",
    "        domain_model_input = X_scaler.transform(domain_model_input)\n",
    "        domain_model_test_data = X_scaler.transform(domain_model_test_data)\n",
    "        adaptation_model_input = X2_scaler.transform(adaptation_model_input)\n",
    "        adaptation_model_test_data = X2_scaler.transform(adaptation_model_test_data)\n",
    "        y_train = y_scaler.transform(y_train)\n",
    "        y_test = y_scaler.transform(y_test) * on_off\n",
    "\n",
    "        ###### MIN MAX CAPACITY ######\n",
    "        mini = np.asarray(test_data[:, [-4, -3]])[-1]\n",
    "        maxi = np.asarray(test_data[:, [-2, -1]])[-1]\n",
    "        mini = y_scaler.transform(mini.reshape(1, -1))[0] * on_off\n",
    "        maxi = y_scaler.transform(maxi.reshape(1, -1))[0] * on_off\n",
    "        #####################\n",
    "        ####################\n",
    "\n",
    "        domain_model_clf.fit(domain_model_input, y_train)\n",
    "        domain_model_pred = domain_model_clf.predict(domain_model_input)\n",
    "        adaptation_model_input = np.concatenate((adaptation_model_input, domain_model_pred), axis=1)\n",
    "        adaptation_model_clf.fit(adaptation_model_input, y_train)\n",
    "\n",
    "        self.xscaler = X_scaler\n",
    "        self.x2scaler = X2_scaler\n",
    "        self.yscaler = y_scaler\n",
    "        self.domain_model = domain_model_clf\n",
    "        self.adaptation_model = adaptation_model_clf\n",
    "        self.mini = mini\n",
    "        self.maxi = maxi\n",
    "        self.on_off = on_off\n",
    "\n",
    "        self.adaptation_model_pred = adaptation_model_clf.predict(np.concatenate([adaptation_model_test_data, domain_model_clf.predict(domain_model_test_data)], axis=1)) * on_off\n",
    "        self.target_substation_pred_data = (self.adaptation_model_pred - np.min(self.adaptation_model_pred, axis=0)) / (\n",
    "                    np.max(self.adaptation_model_pred, axis=0) - np.min(self.adaptation_model_pred, axis=0) + 0.0000000000001) * (maxi - mini) + mini\n",
    "        self.y_test = y_test\n",
    "\n",
    "    def predict(self):\n",
    "        return self.target_substation_pred_data\n",
    "\n",
    "    def score(self, verbose=True):\n",
    "        RMSE = (mean_squared_error(self.y_test, self.target_substation_pred_data)) ** 0.5\n",
    "        R2 = r2_score(self.y_test, self.target_substation_pred_data)\n",
    "        if verbose:\n",
    "            print('RMSE test=', RMSE)\n",
    "            print('R-sqr=', R2)\n",
    "        return RMSE, R2\n",
    "\n",
    "\n",
    "###Create delay/lags (NOT USED HERE, BUT MAY BE USEFUL)\n",
    "n_delay = 1\n",
    "for i in range(len(combined_data)):\n",
    "    delaylist = []\n",
    "    if n_delay > 1:\n",
    "        for n in range(1, n_delay):\n",
    "            delay = combined_data[i].iloc[:, :3].shift(-n)\n",
    "            delay.columns = ['delay' + str(n) + '_' + combined_data[i].columns[0],\n",
    "                             'delay' + str(n) + '_' + combined_data[i].columns[1],\n",
    "                             'delay' + str(n) + '_' + combined_data[i].columns[2]]\n",
    "            delaylist.append(delay)\n",
    "    combined_data[i] = pd.concat([*delaylist, combined_data[i].iloc[:, :]], axis=1).dropna()\n",
    "\n",
    "# CHOOSE THE DATA, METADATA and TARGET, ETC. BY INDEX\n",
    "cc = len(combined_data[0].columns) - 4\n",
    "xindex = list(np.arange(0, n_delay * 3)) + list(np.arange(n_delay * 3 + 2, cc))\n",
    "x2index = list(np.arange(n_delay * 3 + 2, cc))\n",
    "yindex = [n_delay * 3, n_delay * 3 + 1]\n",
    "\n",
    "# PREPARATION\n",
    "ori_combined_data = combined_data.copy()  # Good procedure to prevent data changing in-place\n",
    "clf = KNeighborsRegressor(n_neighbors=20, weights='uniform')  # any model can be specified, this is the domain model\n",
    "clf2 = KNeighborsRegressor(n_neighbors=20,\n",
    "                           weights='uniform')  # any model can be specified, this is the adaptation model\n",
    "\n",
    "nn = len(station_name)\n",
    "for n in range(nn):  # loop through all stations (leave one out)\n",
    "    print(station_name[n])\n",
    "    model = DAZLS()  # Initialize DAZLS model\n",
    "    model.fit(data=ori_combined_data, xindex=xindex, x2index=x2index, yindex=yindex, n=n, domain_model_clf=clf, adaptation_model_clf=clf2,\n",
    "              n_delay=n_delay, cc=cc)  # Fit model\n",
    "    y = model.predict()  # get predicted y\n",
    "    model.score()  # print prediction performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6e7f18c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open('dazls_stored.pkl', 'wb') as model_file:\n",
    "    pickle.dump(model, model_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "aa3040ac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DAZLS()"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open('dazls_stored.pkl', 'rb') as model_file:\n",
    "    loaded_model = pickle.load(model_file)\n",
    "loaded_model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "293a7e08",
   "metadata": {},
   "source": [
    "# IMPROVED DAZLS MODEL"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d10bc7b",
   "metadata": {},
   "source": [
    "This improved version of the **DAZLS** model contains only two out of three steps of the initial model:\n",
    "1. Domain model\n",
    "2. Adaptation model\n",
    "\n",
    "The physical correction model is not being used. Also, the initial model leaves one substation (\"HAL\") out. This imporved model uses all ten substations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "235c93c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error\n",
    "import glob\n",
    "import pandas as pd\n",
    "import os\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.decomposition import FastICA\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler, MaxAbsScaler, RobustScaler, Normalizer, normalize\n",
    "from sklearn.linear_model import Lasso, MultiTaskLassoCV\n",
    "from sklearn.ensemble import RandomForestRegressor, AdaBoostRegressor\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.gaussian_process import GaussianProcessRegressor\n",
    "from sklearn.decomposition import KernelPCA, PCA\n",
    "import random\n",
    "from sklearn.utils import shuffle\n",
    "from scipy.linalg import fractional_matrix_power\n",
    "from sklearn.covariance import LedoitWolf, OAS\n",
    "from sklearn.base import BaseEstimator\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1dcffc18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Seed, path, etc,\n",
    "random.seed(999)\n",
    "np.random.seed(999)\n",
    "\n",
    "path = os.path.dirname(os.path.abspath(\"prep_data\"))\n",
    "folder = ['\\\\prep_data\\\\']\n",
    "combined_data = []\n",
    "station_name = []\n",
    "\n",
    "# Read prepared data\n",
    "for file_name in glob.glob(path + folder[0] + '*.csv'):\n",
    "    x = pd.read_csv(file_name, low_memory=False, parse_dates=[\"datetime\"])\n",
    "    x[\"datetime\"] = pd.to_datetime(x[\"datetime\"])\n",
    "    x = x.set_index('datetime')\n",
    "    x.columns=[x.lower() for x in x.columns]\n",
    "    combined_data.append(x)\n",
    "    sn = os.path.basename(file_name)\n",
    "    station_name.append(sn[:len(sn) - 4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1d7b4269",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the new and improved DAZLS\n",
    "# DAZLS algorithm\n",
    "class BETTER_DAZLS(BaseEstimator):\n",
    "    \n",
    "    \"\"\"\n",
    "    The model carries out wind and solar power prediction for unseen target substations using training data from other substations with\n",
    "    known components.\n",
    "\n",
    "    Any data-driven model can be plugged and used as the base for the domain and the adaptation model.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.__name__ = \"DAZLS\"\n",
    "        self.xscaler = None\n",
    "        self.x2scaler = None\n",
    "        self.yscaler = None\n",
    "        self.domain_model = KNeighborsRegressor(n_neighbors=20,\n",
    "                                  weights='uniform')  # any model can be specified, this is the domain model\n",
    "        self.adaptation_model = KNeighborsRegressor(n_neighbors=20,\n",
    "                                   weights='uniform')  # any model can be specified, this is the adaptation model\n",
    "        self.mini = None\n",
    "        self.maxi = None\n",
    "        self.on_off = None\n",
    "\n",
    "        self.domain_model_input_columns_index = [\"radiation\", \"windspeed_100m\", \"total_substation\", \"lat\", \"lon\", \"solar_on\",\n",
    "                                        \"wind_on\", \"hour\", \"minute\", \"var0\", \"var1\", \"var2\", \"sem0\", \"sem1\"]\n",
    "        self.adaptation_model_input_columns_index = [\"total_substation\", \"lat\", \"lon\", \"solar_on\", \"wind_on\", \"hour\",\n",
    "                                            \"minute\", \"var0\", \"var1\", \"var2\", \"sem0\", \"sem1\"]\n",
    "        self.target_columns_index = [\"total_wind_part\", \"total_solar_part\"]\n",
    "\n",
    "    def fit(self, features, target):\n",
    "        \n",
    "        \"\"\"\n",
    "        In this function we scale the input of the domain and adaptation models of the DAZLS MODEL.\n",
    "        Then we fit the two models.\n",
    "        With the help of the index we separate the features into domain_model_input, adaptation_model_input and the\n",
    "        target, and we use them for the fitting and the training of the models.\n",
    "\n",
    "        :param features: inputs for domain and adaptation model (domain_model_input, adaptation_model_input)\n",
    "        :param target: the expected output (y_train)\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "        #Create delay\n",
    "        #n_delay = 1\n",
    "\n",
    "        #on_off = np.asarray(combined_data[n].iloc[:, [n_delay * 3 + 4, n_delay * 3 + 5]])\n",
    "\n",
    "        X, X2, y = features.loc[:, self.domain_model_input_columns_index], features.loc[:, self.adaptation_model_input_columns_index], target.loc[:, self.target_columns_index]\n",
    "        domain_model_input, adaptation_model_input, y_train = shuffle(X, X2, y, random_state=999)  # just shuffling\n",
    "\n",
    "        xscaler = MinMaxScaler(clip=True)\n",
    "        x2scaler = MinMaxScaler(clip=True)\n",
    "        yscaler = MinMaxScaler(clip=True)\n",
    "        X_scaler = xscaler.fit(domain_model_input)\n",
    "        X2_scaler = x2scaler.fit(adaptation_model_input)\n",
    "        y_scaler = yscaler.fit(y_train)\n",
    "        domain_model_input = X_scaler.transform(domain_model_input)\n",
    "        adaptation_model_input = X2_scaler.transform(adaptation_model_input)\n",
    "        y_train = y_scaler.transform(y_train)\n",
    "\n",
    "        self.domain_model.fit(domain_model_input, y_train)\n",
    "        domain_model_pred = self.domain_model.predict(domain_model_input)\n",
    "        adaptation_model_input = np.concatenate((adaptation_model_input, domain_model_pred), axis=1)\n",
    "        self.adaptation_model.fit(adaptation_model_input, y_train)\n",
    "\n",
    "        self.xscaler = X_scaler\n",
    "        self.x2scaler = X2_scaler\n",
    "        self.yscaler = y_scaler\n",
    "        \n",
    "    def predict(self, test_features):\n",
    "        \n",
    "        \"\"\"\n",
    "        For the prediction we use the test data. We use the index to separate the test data for both domain and\n",
    "        adaptation models.\n",
    "\n",
    "        :param test_features: domain_model_test_data, adaptation_model_test_data\n",
    "        :return: unscaled_test_prediction. The output prediction after both models.\n",
    "        \"\"\"\n",
    "\n",
    "        domain_model_test_data, adaptation_model_test_data = test_features.loc[:, self.domain_model_input_columns_index], test_features.loc[:, self.adaptation_model_input_columns_index]\n",
    "\n",
    "        # Rescale the test_features (if required)\n",
    "        domain_model_test_data_scaled = self.xscaler.transform(domain_model_test_data)\n",
    "        adaptation_model_test_data_scaled = self.x2scaler.transform(adaptation_model_test_data)\n",
    "\n",
    "        # Use the scaled_test_features to make domain_model_prediction\n",
    "        domain_model_test_data_pred = self.domain_model.predict(domain_model_test_data_scaled)\n",
    "\n",
    "        # Use the domain_model_prediction to make adapatation_model_prediction\n",
    "        adaptation_model_test_data_pred = self.adaptation_model.predict(np.concatenate([adaptation_model_test_data_scaled, domain_model_test_data_pred], axis=1))\n",
    "        \n",
    "\n",
    "        # Rescale adaptation_model_prediction (if reqquired)\n",
    "        unscaled_test_prediction = self.yscaler.inverse_transform(adaptation_model_test_data_pred)\n",
    "\n",
    "        #physical correction module (maybe we ll need it in the future)\n",
    "        #target_substation_pred_data = unscaled_test_prediction * self.on_off\n",
    "\n",
    "        return unscaled_test_prediction\n",
    "    \n",
    "    def score(self, truth, prediction):\n",
    "        \n",
    "        \"\"\"\n",
    "        Evaluation of the prediction's output.\n",
    "\n",
    "        :param truth: real values\n",
    "        :param prediction: predicted values\n",
    "\n",
    "        :return: RMSE and R2 scores\n",
    "        \"\"\"\n",
    "\n",
    "        RMSE = (mean_squared_error(truth, prediction)) ** 0.5\n",
    "        R2 = r2_score(truth, prediction)\n",
    "        return RMSE, R2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "548940ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing station: HAL\n"
     ]
    }
   ],
   "source": [
    "# Lets use the first substation for testing\n",
    "\n",
    "# Split data in train and test\n",
    "training_data = pd.concat(ori_combined_data[1:])\n",
    "test_data = ori_combined_data[0]\n",
    "target_columns =['total_solar_part', 'total_wind_part']\n",
    "feature_columns = [x for x in test_data.columns if x not in target_columns]\n",
    "print('Testing station:',station_name[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8b887ce3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2.7766225853266904, -0.3705595044428144)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Initialize DAZLS model\n",
    "model = BETTER_DAZLS() \n",
    "# Fit model\n",
    "model.fit(training_data.loc[:,feature_columns], training_data.loc[:,target_columns])\n",
    "\n",
    "y = model.predict(test_data.loc[:,feature_columns])  # get predicted y\n",
    "model.score(test_data.loc[:,target_columns], y)  # print prediction performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f89023f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = test_data.loc[:,target_columns].copy()\n",
    "result['wind_split'] = y[:,0]\n",
    "result['solar_split'] = y[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f682ed4",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "result.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e0376e3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "result.iloc[60:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f057e19",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open('better_dazls_stored.pkl', 'wb') as model_file:\n",
    "    pickle.dump(model, model_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36ef1c9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('better_dazls_stored.pkl', 'rb') as model_file:\n",
    "    loaded_model = pickle.load(model_file)\n",
    "loaded_model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b75c5815",
   "metadata": {},
   "source": [
    "**COMPARISON BETWEEN DAZLS AND BETTER DAZLS RMSE AND R2 SCORES**\n",
    "\n",
    "The two following sells were used to generate the metrics for the better-dazls model. Out of the ten csv files which contain the substations' details, every time we used one for the test_data and the rest were combined for the training_data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "71f2297b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing station: WWF\n"
     ]
    }
   ],
   "source": [
    "training_data = pd.concat([ori_combined_data[0],ori_combined_data[1],ori_combined_data[2],ori_combined_data[3],ori_combined_data[4],ori_combined_data[5],ori_combined_data[6],ori_combined_data[7],ori_combined_data[8]])\n",
    "test_data = ori_combined_data[9]\n",
    "target_columns =['total_solar_part', 'total_wind_part']\n",
    "feature_columns = [x for x in test_data.columns if x not in target_columns]\n",
    "print('Testing station:',station_name[9])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "590d7454",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2.5052706068558024, -0.4181292614980786)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Initialize DAZLS model\n",
    "model = BETTER_DAZLS() \n",
    "# Fit model\n",
    "model.fit(training_data.loc[:,feature_columns], training_data.loc[:,target_columns])\n",
    "\n",
    "y = model.predict(test_data.loc[:,feature_columns])  # get predicted y\n",
    "model.score(test_data.loc[:,target_columns], y)  # print prediction performance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01c81b10",
   "metadata": {},
   "source": [
    "# DAZLS SCORES\n",
    "\n",
    "**HAL**\n",
    "\n",
    "RMSE test= 0.03685411732317427\n",
    "\n",
    "R-sqr= 0.30408593867613676\n",
    "\n",
    "**HFDP**\n",
    "\n",
    "RMSE test= 0.29731055878988316\n",
    "\n",
    "R-sqr= -1.3908466829338044\n",
    "\n",
    "**LLS**\n",
    "\n",
    "RMSE test= 0.18550646112011965\n",
    "\n",
    "R-sqr= 0.6928684901176348\n",
    "\n",
    "**MNZL**\n",
    "\n",
    "RMSE test= 0.06787005934175006\n",
    "\n",
    "R-sqr= 0.2971844944391836\n",
    "\n",
    "**NRYN**\n",
    "\n",
    "RMSE test= 0.060716139554765194\n",
    "\n",
    "R-sqr= 0.692598476068661\n",
    "\n",
    "**OWD**\n",
    "\n",
    "RMSE test= 0.0983913348736813\n",
    "\n",
    "R-sqr= 0.8999194024661905\n",
    "\n",
    "**WEW**\n",
    "\n",
    "RMSE test= 0.0879411676678399\n",
    "\n",
    "R-sqr= 0.5557680473477729\n",
    "\n",
    "**WHF**\n",
    "\n",
    "RMSE test= 0.031980482086807246\n",
    "\n",
    "R-sqr= 0.8101134986300711\n",
    "\n",
    "**WLS**\n",
    "\n",
    "RMSE test= 0.04693656013938843\n",
    "\n",
    "R-sqr= 0.7355097726581723\n",
    "\n",
    "**WWF**\n",
    "\n",
    "RMSE test= 0.03471334262302751\n",
    "\n",
    "R-sqr= 0.7062994479576562"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1022f3d9",
   "metadata": {},
   "source": [
    "# BETTER DAZLS SCORES\n",
    "\n",
    "**HAL**\n",
    "\n",
    "RMSE = 2.7766225853266904\n",
    "\n",
    "R2 = -0.3705595044428144\n",
    "\n",
    "**HFDP**\n",
    "\n",
    "RMSE = 6.471371844978199\n",
    "\n",
    "R2 = -0.20079672597529474\n",
    "\n",
    "**LLS**\n",
    "\n",
    "RMSE = 4.70581079058933\n",
    "\n",
    "R2 = -0.33895840468691185\n",
    "\n",
    "**MNZL**\n",
    "\n",
    "RMSE = 7.10860840811325\n",
    "\n",
    "R2 = -0.4013474321552215\n",
    "\n",
    "**NRYN**\n",
    "\n",
    "RMSE = 4.477375026144818\n",
    "\n",
    "R2 = -0.61359470553818\n",
    "\n",
    "**0WD**\n",
    "\n",
    "RMSE = 7.895994438783822\n",
    "\n",
    "R2 = -0.21529024204614222\n",
    "\n",
    "**WEW**\n",
    "\n",
    "RMSE = 3.680244889349275\n",
    "\n",
    "R2 =  -0.5394136893557131\n",
    "\n",
    "**WHF**\n",
    "\n",
    "RMSE = 9.413476036850211\n",
    "\n",
    "R2 = -0.2242081300163583)\n",
    "\n",
    "**WLS**\n",
    "\n",
    "RMSE = 2.3035715484208423\n",
    "\n",
    "R2 = -0.482455499103511\n",
    "\n",
    "**WWF**\n",
    "\n",
    "RMSE = 2.5052706068558024\n",
    "\n",
    "R2 = -0.4181292614980786"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d9d0dcc",
   "metadata": {},
   "source": [
    "**RMSE**\n",
    "Comparing the RMSE scores of the two models we can observe that the initial dazls model has better scores from the better-dazls model.\n",
    "\n",
    "**R2**\n",
    "Comparing the R2 scores we observe that almost all dazls scores are better than better-dazls score, except of HFDP scores."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

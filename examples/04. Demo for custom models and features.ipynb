{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7b83a4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5a7f3e0",
   "metadata": {},
   "source": [
    "This notebook assumes jupytext is installed so that it can be used as a script (convenient to use debugging tools)\n",
    "\n",
    "This also assumes the modifications added in the branch: `demo-custom-model-and-features`\n",
    "\n",
    "This explains the apparent mis-ordering of import statements, since we prefer having them in the cell that uses the imports."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f101934",
   "metadata": {},
   "source": [
    "## Needed feature\n",
    "\n",
    "Possibility to specify a feature set that is different from the one computed by OpenSTEF."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ce206d2",
   "metadata": {},
   "source": [
    "## Involved modifications\n",
    "\n",
    "* Added a `default_modelspecs` field to the **prediction job data class**\n",
    "* Extraction of the default prediction job in the **train task**\n",
    "* Added a `default_modelspecs` argument to the input function of the **train pipeline** and its usage.\n",
    "\n",
    "We can also deal with configs differently :\n",
    "\n",
    "* Adding defaults in the config manager\n",
    "* Additional optional method to the db connection. `db.get_default_modelspecs(pj_id)`\n",
    "* Add some or all the new fields to the model specifications.\n",
    "\n",
    "\n",
    "Note that this could solve also some parts of the **proloaf specific code**. For instance:\n",
    "\n",
    "* [Specific conditions in the feature application](https://github.com/OpenSTEF/openstef/blob/f204eccb8dea85b7236e5ebbdb23ffd6ceefac03/openstef/feature_engineering/feature_applicator.py#L122)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71a5e7aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a94cab1e",
   "metadata": {},
   "source": [
    "## Simple custom model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c13027f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openstef.model.regressors.custom_regressor import CustomOpenstfRegressor\n",
    "from openstef.model.regressors.regressor import OpenstfRegressor\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "class CustomLinear(CustomOpenstfRegressor):\n",
    "    def __init__(self, strategy=\"mean\"):\n",
    "        self.strategy = strategy\n",
    "        self._feature_names = None\n",
    "        \n",
    "    @property\n",
    "    def feature_names(self):\n",
    "        self._feature_names\n",
    "\n",
    "    @classmethod\n",
    "    def valid_kwargs(cls):\n",
    "        return [\"strategy\"]\n",
    "    \n",
    "    def fit(self, X, y, **fit_options):\n",
    "        self._feature_names = list(X.columns)\n",
    "        self.pipeline_ = Pipeline([\n",
    "            (\"imputer\", SimpleImputer(strategy=self.strategy)),\n",
    "            (\"linear\", LinearRegression())\n",
    "        ]).fit(X, y)\n",
    "        self.feature_importances_ = list(self.pipeline_[\"linear\"].coef_)\n",
    "        return self\n",
    "    \n",
    "    def predict(self, X):\n",
    "        return self.pipeline_.predict(X)\n",
    "    \n",
    "    def objective(self):\n",
    "        return None\n",
    "    \n",
    "    @staticmethod\n",
    "    def _get_importance_names():\n",
    "        return {\n",
    "            \"gain_importance_name\": \"total_gain\",\n",
    "            \"weight_importance_name\": \"weight\",\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa394e16",
   "metadata": {},
   "source": [
    "## Dummy extra feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cab2b707",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openstef.feature_engineering.feature_adder import FeatureAdder\n",
    "\n",
    "class RandomFeature(FeatureAdder):\n",
    "    @property\n",
    "    def name(self):\n",
    "        return \"random\"\n",
    "\n",
    "    @property\n",
    "    def _regex(self):\n",
    "        return \"^random$\"\n",
    "    \n",
    "    def required_features(self, feature_names):\n",
    "        return []\n",
    "    \n",
    "    def apply_features(self, df, parsed_feature_names):\n",
    "        return df.assign(random=np.random.rand(len(df)))\n",
    "\n",
    "\n",
    "class FeatureCopy(FeatureAdder):\n",
    "    @property\n",
    "    def name(self):\n",
    "        return \"feature_copy\"\n",
    "\n",
    "    @property\n",
    "    def _regex(self):\n",
    "        return r\"^(?P<feature_name>.*)_copy$\"\n",
    "    \n",
    "    def required_features(self, feature_names):\n",
    "        return []\n",
    "    \n",
    "    def apply_features(self, df, parsed_feature_names):\n",
    "        return df.assign(\n",
    "                **{\n",
    "                    feature.name: df[feature.params[\"feature_name\"]]\n",
    "                    for feature in parsed_feature_names\n",
    "                }\n",
    "            )\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4d426c8",
   "metadata": {},
   "source": [
    "## Mocking functionalities\n",
    "\n",
    "### Unactivate Teams connection\n",
    "First we cancel the functionality to send messages via teams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e59cd901",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openstef.tasks.utils.taskcontext import TaskContext\n",
    "\n",
    "# Dirty patch Taskcontext to skip the teams functionality\n",
    "def dummy_func(*args, **kwargs):\n",
    "    pass\n",
    "\n",
    "TaskContext._send_teams_message = dummy_func"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f70e466",
   "metadata": {},
   "source": [
    "### Fake database connection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "356f799b",
   "metadata": {},
   "source": [
    "A function to transform json prediction jobs into an PJ object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e22d5e70",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openstef.data_classes.prediction_job import PredictionJobDataClass\n",
    "from openstef.data_classes.model_specifications import ModelSpecificationDataClass\n",
    "\n",
    "def parse_pj_json(pj_json):\n",
    "    return PredictionJobDataClass(\n",
    "        id=pj_json[\"id\"],\n",
    "        model=pj_json[\"model\"],\n",
    "        forecast_type=pj_json[\"typ\"],\n",
    "        name=pj_json[\"typ\"],\n",
    "        horizon_minutes=pj_json[\"horizon_minutes\"],\n",
    "        resolution_minutes=pj_json[\"resolution_minutes\"],\n",
    "        lat=pj_json[\"lat\"],\n",
    "        lon=pj_json[\"lon\"],\n",
    "        train_components=pj_json[\"train_components\"],\n",
    "        description=pj_json[\"description\"],\n",
    "        quantiles=pj_json[\"quantiles\"],\n",
    "        default_modelspecs = ModelSpecificationDataClass(\n",
    "            id=pj_json[\"id\"],\n",
    "            hyper_params=pj_json[\"hyper_params\"],\n",
    "            feature_names=pj_json[\"feature_names\"],\n",
    "            feature_modules=pj_json.get(\"feature_modules\", [])\n",
    "        )\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53872c39",
   "metadata": {},
   "source": [
    "We create a test prediction job (the one for which reference data exist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c574ab1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load test prediction jobs\n",
    "with open(\"../test/unit/data/prediction_jobs.json\") as infile:\n",
    "    pjs = json.load(infile)\n",
    "    \n",
    "pj_307 = parse_pj_json(pjs[\"307\"])\n",
    "\n",
    "# Use a custom model\n",
    "pj_307.model = \"__main__.CustomLinear\"\n",
    "\n",
    "# Use custom features\n",
    "pj_307.default_modelspecs.feature_names = (\n",
    "    pj_307.default_modelspecs.feature_names[:20] + \n",
    "    [\"random\", \"horizon_copy\"]\n",
    ")\n",
    "pj_307.default_modelspecs.feature_modules = [\"__main__\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da4ff465",
   "metadata": {},
   "source": [
    "We load the data returned by the fake database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e99636a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load test data\n",
    "df = pd.read_csv(\"../test/unit/data/reference_sets/307-train-data.csv\", index_col=0, parse_dates=[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30e676c7",
   "metadata": {},
   "source": [
    "The fake database connection class:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1f2e23d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "\n",
    "class DataBaseConnection:\n",
    "    def __init__(self, pj, data):\n",
    "        self.pj = pj\n",
    "        self.data = data\n",
    "\n",
    "    def get_prediction_jobs(self, **kwargs):\n",
    "        return [copy.deepcopy(self.pj)]\n",
    "\n",
    "    def get_model_input(self, *args, **kwargs):\n",
    "        return self.data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c52eadf",
   "metadata": {},
   "source": [
    "We instantiate the fake database:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a508043",
   "metadata": {},
   "outputs": [],
   "source": [
    "db = DataBaseConnection(pj_307, df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "900f2fc6",
   "metadata": {},
   "source": [
    "### Fake configuration manager\n",
    "\n",
    "We create a fake connection manager with default model specs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66d326a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils import Bunch\n",
    "\n",
    "config = Bunch(**{\n",
    "    \"teams\": Bunch(**{\"monitoring_url\": \"\"}),\n",
    "    \"proxies\": [],\n",
    "    \"paths\": Bunch(\n",
    "        trained_models_folder=\"trained/\"\n",
    "    ),\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c97290b",
   "metadata": {},
   "source": [
    "## Running the train model task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cdb2512",
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "import glob\n",
    "from openstef.tasks.train_model import main as train_main\n",
    "from openstef.model.serializer import MLflowSerializer\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Remove previously trained models\n",
    "    for d in glob.glob(\"trained/mlruns/*\"):\n",
    "        shutil.rmtree(d)\n",
    "\n",
    "    requested_features = pj_307.default_modelspecs.feature_names[:]\n",
    "    \n",
    "    train_main(\"test_pby\", config=config, database=db)\n",
    "    \n",
    "    # Checking initial features\n",
    "    init_features = list(db.get_model_input(pj_307[\"id\"]))\n",
    "\n",
    "    # Checking final features\n",
    "    s = MLflowSerializer(\"./trained/\")\n",
    "    model, specs = s.load_model(pj_307[\"id\"])\n",
    "    final_features = specs.feature_names\n",
    "    \n",
    "    # Retrieving requested features\n",
    "    \n",
    "    print(\">>>>>> Initial features: \", init_features)\n",
    "    print(\">>>>>> Requested features: \", requested_features)\n",
    "    print(\">>>>>> Final features:\", final_features)\n",
    "    print(\">>>>>> Features difference:\", set(requested_features) ^ set(specs.feature_names))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e67e7612",
   "metadata": {},
   "source": [
    "## Additional notes\n",
    "\n",
    "* The horizon information in the PJ is not used at train time. Some default horizons are used. We should be able to config this.\n",
    "* We should also be able to use no horizons if they already come from the input data (for instance because we need to combine with weather forecasts).\n",
    "* At forecast we cannot specify a set of horizons which does not start from now.\n",
    "* The start time is computed when the specific PJ is treated. This means that PJ's treated by the same task can have different train data sets.\n",
    "* The database interface should be declared in OpenSTEF, since it used in tasks\n",
    "* Is there a config manager interface ?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cff6dbc",
   "metadata": {},
   "source": [
    "## Configuration needs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c83b9c22",
   "metadata": {},
   "source": [
    "* `train_horizons = None`, with `[]` we can skip the horizon duplication mechanism when data base returns forecasts as features (we do not need it for RTE).\n",
    "* `train_split_func: str = None` would also need a start time passed by the task (we probably need it).\n",
    "* `default_modelspecs: ModelSpecificationsDataObject = None`\n",
    "* `depends_on = []`: the list of prediction jobs the job depends on (we need it)\n",
    "* `save_train_forecasts`: to enable the possibility to save intermediate results across prediction jobs (we need it)\n",
    "\n",
    "Can be implemented as:\n",
    "\n",
    "* Additional fields into the PJ data class\n",
    "* Optional DB query\n",
    "* Optional entries in the config manager"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b1e169e",
   "metadata": {},
   "source": [
    "## Tasks prediction job loops and dependencies\n",
    "\n",
    "We illustrate a possible algorithm to order tasks so that they respect some dependency constraints.\n",
    "\n",
    "* Group prediction jobs taking into account dependencies\n",
    "* Iterate on groups\n",
    "\n",
    "Example of algorithm to compute groups:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "486ee41a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from random import shuffle\n",
    "import networkx as nx\n",
    "\n",
    "def build_graph(pjs):\n",
    "    nodes = set()\n",
    "    edges = set()\n",
    "    \n",
    "    for pj in pjs:\n",
    "        nodes.add(pj[\"id\"])\n",
    "        for pj_dep in pj.get(\"depends_on\", []):\n",
    "            nodes.add(pj_dep)\n",
    "            edges.add((pj[\"id\"], pj_dep))\n",
    "            \n",
    "    return nodes, edges\n",
    "\n",
    "def _update_parent_level(parents, levels, child, parent):\n",
    "    \"\"\"Recursively updates parent levels\n",
    "    \"\"\"\n",
    "    child_level = levels[child]\n",
    "    parent_level = levels[parent]\n",
    "    \n",
    "    if parent_level <= child_level:\n",
    "        levels[parent] = child_level + 1\n",
    "        grand_parents = parents[parent]\n",
    "        for gp in grand_parents:\n",
    "            _update_parent_level(parents, levels, parent, gp)\n",
    "\n",
    "\n",
    "def group_nodes(nodes, edges):\n",
    "    if has_cycles(edges):\n",
    "        raise ValueError(\"Invalid graph structure. The graph has cycles or back edges\")\n",
    "\n",
    "    parents_dict = {\n",
    "        node: set() for node in nodes\n",
    "    }\n",
    "    \n",
    "    for parent, child in edges:\n",
    "        parents = parents_dict[child]\n",
    "        parents.add(parent)\n",
    "\n",
    "    levels = {n: 0 for n in nodes}\n",
    "        \n",
    "    for child, parents in parents_dict.items():\n",
    "        for parent in parents:\n",
    "            _update_parent_level(parents_dict, levels, child, parent)\n",
    "            \n",
    "    groups = {}\n",
    "    \n",
    "    for node, level in levels.items():\n",
    "        group_nodes = groups.setdefault(level, [])\n",
    "        group_nodes.append(node)\n",
    "            \n",
    "    return groups\n",
    "            \n",
    "def has_cycles(edges):\n",
    "    graph = nx.DiGraph()\n",
    "    graph.add_edges_from(edges)\n",
    "    try:\n",
    "        nx.find_cycle(graph)\n",
    "        return True\n",
    "    except nx.NetworkXNoCycle:\n",
    "        return False\n",
    "\n",
    "def draw_graph(nodes, edges):\n",
    "    graph = nx.DiGraph()\n",
    "    graph.add_edges_from(edges)\n",
    "    graph.add_nodes_from(nodes)\n",
    "    nx.draw(graph, with_labels=True, arrows=True)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb12a064",
   "metadata": {},
   "outputs": [],
   "source": [
    "pjs = [\n",
    "    {\"id\":1},\n",
    "    {\"id\":2},\n",
    "    {\"id\":3},\n",
    "    {\"id\":4, \"depends_on\":[1, 2]},\n",
    "    {\"id\":5, \"depends_on\":[1, 3]},\n",
    "    {\"id\":6, \"depends_on\":[4]},\n",
    "    {\"id\":7}\n",
    "]\n",
    "\n",
    "shuffle(pjs)\n",
    "\n",
    "nodes, edges = build_graph(pjs)\n",
    "print(\"Has cycles?\", has_cycles(edges))\n",
    "print(group_nodes(nodes, edges))\n",
    "\n",
    "draw_graph(nodes, edges)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b716943f",
   "metadata": {},
   "source": [
    "The idea would then be to loop on groups and for each perform the usual prediction job loop if we assume the possibility to run prediction jobs in parallel or simply sort the prediction jobs otherwise.\n",
    "\n",
    "Should we create a specific task ?\n",
    "\n",
    "We must raise an error if cycles are present:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c20136b",
   "metadata": {},
   "outputs": [],
   "source": [
    "pjs = [\n",
    "    {\"id\":1, \"depends_on\": [6]},\n",
    "    {\"id\":2},\n",
    "    {\"id\":3},\n",
    "    {\"id\":4, \"depends_on\":[1, 2]},\n",
    "    {\"id\":5, \"depends_on\":[1, 3]},\n",
    "    {\"id\":6, \"depends_on\":[4]}\n",
    "]\n",
    "\n",
    "nodes, edges = build_graph(pjs)\n",
    "draw_graph(nodes, edges)\n",
    "print(\"Has cycles?\", has_cycles(edges))\n",
    "print(group_nodes(nodes, edges))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "495413c2",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "import random\n",
    "n_pjs = 2000\n",
    "p_link = 0.01\n",
    "pjs = []\n",
    "for i in range(n_pjs):\n",
    "    pj = {\"id\": i}\n",
    "    for j in range(i):\n",
    "        if random.random() < p_link:\n",
    "            deps = pj.setdefault(\"depends_on\", [])\n",
    "            deps.append(j)\n",
    "    pjs.append(pj)        \n",
    "\n",
    "pjs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bfce964",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%prun\n",
    "nodes, edges = build_graph(pjs)\n",
    "group_nodes(nodes, edges)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8c7ec54",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "formats": "ipynb,auto:light",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Evaluate performance (quality) of openstf models using Backtest Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "This notebook generates a comparison of the performance of different models/hyperparameters/forecasting horizons for default data sets. Convenient to compare a new type of model to an existing model. \n",
    "This is still a preliminary version."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-09T16:49:40.438572Z",
     "start_time": "2022-02-09T16:49:40.417603Z"
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import plotly.express as px\n",
    "\n",
    "from openstef.pipeline.train_create_forecast_backtest import train_model_and_forecast_back_test\n",
    "from openstef.metrics import metrics\n",
    "from openstef.data_classes.model_specifications import ModelSpecificationDataClass\n",
    "from openstef.data_classes.prediction_job import PredictionJobDataClass # TODO, import from openstef when availavle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Set the variables in order to determine what are the:\n",
    "- *id's* for the standard datasets\n",
    "- *id* for the new dataset (make sure it is unknown for training model on your computer)\n",
    "- old model to compare ('xgb', 'lgb', 'xgb_quantile')\n",
    "- new model\n",
    "- hyperparameter settings for the old and new model, you can leave empty if no changes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-09T16:48:58.400981Z",
     "start_time": "2022-02-09T16:48:58.392984Z"
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "PJ_ID_STD = [287, 307] #By default, only test these two for speed. Additional options: 435, 438\n",
    "PJ_ID_NEW = 12345\n",
    "PJ_MODEL_OLD = 'xgb'\n",
    "PJ_MODEL_NEW = 'xgb_quantile'\n",
    "PJ_HYPER_PARAMS_OLD = {}\n",
    "PJ_HYPER_PARAMS_NEW = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Comparing the models, using metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-09T16:52:02.974570Z",
     "start_time": "2022-02-09T16:52:02.952543Z"
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def pj_model_metrics (pj_id: int, pj_model: str, pj_hyper_params: dict, data: pd.DataFrame) -> pd.DataFrame:\n",
    "\n",
    "    \"\"\"\n",
    "    Retrieve the metrics related to the model and the prediction job\n",
    "\n",
    "    Args:\n",
    "        pj_id (int): prediction job id\n",
    "        pj_model (str): prediction job model\n",
    "        pj_hyper_params (dict): prediction job hyper parameters\n",
    "        data (pd.DataFrame): dataframe as input for training model\n",
    "    Returns:\n",
    "        (pandas.DataFrame): metrics related to the forecasting\n",
    "    \"\"\"\n",
    "\n",
    "    # prediction job\n",
    "    pj=PredictionJobDataClass(id=pj_id,\n",
    "        model=pj_model,\n",
    "        quantiles=[0.10,0.30,0.50,0.70,0.90],\n",
    "        horizon_minutes=48*60,\n",
    "        resolution_minutes=15,\n",
    "        lat = 1, #should become optional\n",
    "        lon = 1, #should become optional\n",
    "        train_components=False,\n",
    "        name='TestPrediction',\n",
    "        model_type_group=None, # Note, this should become optional\n",
    "        hyper_params=pj_hyper_params, # Note, this should become optional\n",
    "        feature_names=None, # Note, this should become optional\n",
    "        forecast_type=\"demand\", # Note, this should become optional\n",
    "                  )\n",
    "    \n",
    "    modelspecs = ModelSpecificationDataClass(id=pj['id'], hyper_params=pj_hyper_params)\n",
    "\n",
    "    # Perform the backtest\n",
    "    forecast, model, train_data, validation_data, test_data = train_model_and_forecast_back_test(\n",
    "        pj,\n",
    "        modelspecs,\n",
    "        data,\n",
    "        training_horizons=[0.25, 47.0],\n",
    "     )\n",
    "\n",
    "    forecast = forecast.drop(columns=['pid', 'customer', 'description', 'type', 'algtype'])\n",
    "\n",
    "    metrics_dict = {}\n",
    "\n",
    "    for horizon in forecast.horizon.unique():\n",
    "        metrics_horizon = {}\n",
    "        horizon_df = forecast[forecast['horizon']==horizon]\n",
    "        horizon_forecast = forecast[forecast['horizon']==horizon]['forecast']\n",
    "        horizon_realised = forecast[forecast['horizon']==horizon]['realised']\n",
    "\n",
    "        metrics_horizon['mae'] = metrics.mae(horizon_realised, horizon_forecast)\n",
    "        metrics_horizon['rmse'] = metrics.rmse(horizon_realised, horizon_forecast)\n",
    "        metrics_horizon['skill_score'] = metrics.skill_score(horizon_realised, horizon_forecast, horizon_realised.mean())\n",
    "        metrics_horizon['r_mae_lowest'] = metrics.r_mae_lowest(horizon_realised, horizon_forecast)\n",
    "        metrics_horizon['r_mae_highest'] = metrics.r_mae_highest(horizon_realised, horizon_forecast)\n",
    "\n",
    "        metrics_horizon['percentage_quantile'] = 100 - \\\n",
    "                                                 (len(horizon_df[\"forecast\"][:][horizon_df[\"forecast\"] > horizon_df[\"realised\"].quantile(0.95)])/len(horizon_df))*100 -\\\n",
    "                                                 (len(horizon_df[\"forecast\"][:][horizon_df[\"forecast\"] < horizon_df[\"realised\"].quantile(0.05)])/len(horizon_df))*100\n",
    "        metrics_dict[f'forecast_{horizon}h_ahead'] = metrics_horizon\n",
    "\n",
    "    return pd.DataFrame.from_dict(metrics_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-09T16:52:05.067008Z",
     "start_time": "2022-02-09T16:52:05.053033Z"
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def compare_models (type_forecast:int) -> pd.DataFrame:\n",
    "\n",
    "    \"\"\"\n",
    "    Compare the new model to the older model\n",
    "\n",
    "    Args:\n",
    "        type_forecast (int): forecasting for 15min or 47h\n",
    "    Returns:\n",
    "        (pandas.DataFrame): difference in metrics between the models\n",
    "    \"\"\"\n",
    "\n",
    "    metrics_dict = {}\n",
    "\n",
    "    if type_forecast==0.25:\n",
    "        idx_forecast_ahead=0\n",
    "    else:\n",
    "        idx_forecast_ahead=1\n",
    "\n",
    "    for i in range(len(PJ_ID_STD)):\n",
    "        metrics_diff = {}\n",
    "\n",
    "        data = pd.read_csv(f'data/get_model_input_pid_{PJ_ID_STD[i]}.csv', index_col='index', parse_dates=True)\n",
    "        metrics_old = pj_model_metrics(PJ_ID_STD[i], PJ_MODEL_OLD, PJ_HYPER_PARAMS_OLD, data)\n",
    "        metrics_new = pj_model_metrics(PJ_ID_NEW+i, PJ_MODEL_NEW, PJ_HYPER_PARAMS_NEW, data)\n",
    "\n",
    "        # the lower the score, the better, so diff metric should be positive if new model better\n",
    "        metrics_diff['mae (old-new)'] = metrics_old.loc['mae',:][idx_forecast_ahead] - \\\n",
    "                              metrics_new.loc['mae',:][idx_forecast_ahead]\n",
    "        metrics_diff['rmse (old-new)'] = metrics_old.loc['rmse',:][idx_forecast_ahead] - \\\n",
    "                              metrics_new.loc['rmse',:][idx_forecast_ahead]\n",
    "        metrics_diff['r_mae_highest (old-new)'] = metrics_old.loc['r_mae_highest',:][idx_forecast_ahead] - \\\n",
    "                              metrics_new.loc['r_mae_highest',:][idx_forecast_ahead]\n",
    "        metrics_diff['r_mae_lowest (old-new)'] = metrics_old.loc['r_mae_lowest',:][idx_forecast_ahead] - \\\n",
    "                              metrics_new.loc['r_mae_lowest',:][idx_forecast_ahead]\n",
    "        metrics_diff['skill_score (old-new)'] = metrics_old.loc['skill_score',:][idx_forecast_ahead] - \\\n",
    "                              metrics_new.loc['skill_score',:][idx_forecast_ahead]\n",
    "        metrics_diff['percentage_quantile_old'] = metrics_old.loc['percentage_quantile',:][idx_forecast_ahead]\n",
    "        metrics_diff['percentage_quantile_new'] = metrics_new.loc['percentage_quantile',:][idx_forecast_ahead]\n",
    "\n",
    "        metrics_dict[f'pj_id {PJ_ID_STD[i]}'] = metrics_diff\n",
    "\n",
    "    pd.DataFrame.from_dict(metrics_dict)\n",
    "\n",
    "    return pd.DataFrame.from_dict(metrics_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-09T16:54:50.219082Z",
     "start_time": "2022-02-09T16:52:06.020284Z"
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "diff_compared_metrics_025 = compare_models(type_forecast=0.25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-09T16:58:53.556664Z",
     "start_time": "2022-02-09T16:56:10.001383Z"
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "diff_compared_metrics_47 = compare_models(type_forecast=47.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "How lower the score (range from 0 to 1), the better, so the difference between the metrics (old-new) should be positive if the new model is better.\n",
    "Looking at the tables and figures, we can see the difference between different metrics and the amount of data (percentage) within in the 95% quantile."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-09T16:58:53.604172Z",
     "start_time": "2022-02-09T16:58:53.558696Z"
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Comparing the models, with a horizon of 15 min\n",
    "diff_compared_metrics_025"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-09T16:58:53.620168Z",
     "start_time": "2022-02-09T16:58:53.607163Z"
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Comparing the models, with a horizon of 47 min\n",
    "diff_compared_metrics_47"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-09T16:58:54.702750Z",
     "start_time": "2022-02-09T16:58:53.623164Z"
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "fig = px.bar(diff_compared_metrics_025[:5],  barmode = 'group',\n",
    "             title= \"Difference between metric values from the old and the new model, 15min. <br><sup>Blue dashed line at 0.1, to check whether difference between old and new model is larger than 0.1<sup>\",\n",
    "             labels={'index':'Metrics','value':'Difference'})\n",
    "fig.add_hline(y=0.1, line_width=1, line_dash=\"dash\", line_color=\"blue\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-09T16:58:54.798244Z",
     "start_time": "2022-02-09T16:58:54.704243Z"
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "fig = px.bar(diff_compared_metrics_47[:5],  barmode = 'group',\n",
    "             title= \"Difference between metric values from the old and the new model, 47h. <br><sup>Blue dashed line at 0.1, to check whether difference between old and new model is larger than 0.1<sup>\",\n",
    "             labels={'index':'Metrics','value':'Difference'})\n",
    "fig.add_hline(y=0.1, line_width=1, line_dash=\"dash\", line_color=\"blue\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-09T16:58:54.893590Z",
     "start_time": "2022-02-09T16:58:54.800245Z"
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "fig = px.bar(diff_compared_metrics_025[5:7],  barmode = 'group',\n",
    "             title = \"Percentage of data within the 95% confidence interval, 15min. <br><sup> Blue dashed line at 95% to check whether amount of data within confidence interval, is around the 95% <sup>\",\n",
    "             labels={'index':'','value':'Percentage %'}, range_y=[75,100])\n",
    "fig.add_hline(y=95, line_width=1, line_dash=\"dash\", line_color=\"blue\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-09T16:58:55.036788Z",
     "start_time": "2022-02-09T16:58:54.895591Z"
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "fig = px.bar(diff_compared_metrics_47[5:7],  barmode = 'group',\n",
    "             title = \"Percentage of data within the 95% confidence interval, 47h. <br><sup>Blue dashed line at 95% to check whether amount of data within confidence interval, is around the 95%<sup>\",\n",
    "             labels={'index':'','value':'Percentage %'}, range_y=[75,100])\n",
    "fig.add_hline(y=95, line_width=1, line_dash=\"dash\", line_color=\"blue\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}

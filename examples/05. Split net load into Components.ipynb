{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "64765210",
   "metadata": {},
   "source": [
    "# Dazls model\n",
    "\n",
    "Energy splitting method in openSTEF\n",
    "\n",
    "It trains one splitting model which can be used for every prediction job. As input it uses data from multiple substations with known components and it outputs the prediction of solar and wind power for unkown target substations.\n",
    "\n",
    "The model contains 2-steps which are deployed in sequence:\n",
    "1. Domain model (any data-driven model can be used)\n",
    "2. Adaptation model (any data-driven model can be used)\n",
    "\n",
    "#### For reference, see: [dazls.rst](https://github.com/OpenSTEF/openstef/tree/main/docs/dazls.rst)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69f0a54a",
   "metadata": {},
   "source": [
    "### This notebook contains the code to:\n",
    "\n",
    "1. preprocess the data which are going to be used for the model\n",
    "\n",
    "We use the \"combined_data\" folder, which contains the raw data, to preprocess the data and save it in the folder \"prep_data\". After this, the preprocessed data with metadata can be found in the \"prep_data\" file in the path we have set. Then we use the prep_data to run the dazls model.\n",
    "\n",
    "2. train dazls and generate a prediction for 1 out-of-sample csv file \n",
    "\n",
    "\n",
    "3. load and store the model\n",
    "\n",
    "The dazls_stored.sav file is being produced and is being used in openstef for the components forecast ([create_component_forecast](https://github.com/OpenSTEF/openstef/blob/main/openstef/pipeline/create_component_forecast.py))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7b4284a",
   "metadata": {},
   "source": [
    "#####  Preprocess the data. Create the \"prep_data\" folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "056a49f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error\n",
    "import glob\n",
    "import pandas as pd\n",
    "import os\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "import random \n",
    "from sklearn.utils import shuffle\n",
    "import joblib\n",
    "\n",
    "#Seed and preparation\n",
    "random.seed(999)\n",
    "np.random.seed(999)\n",
    "\n",
    "#Path\n",
    "combined_data=[]\n",
    "station_name=[]\n",
    "\n",
    "\n",
    "#Read, create metadata and save in prep_data folder\n",
    "for file_name in glob.glob('combined_data/*.csv'):\n",
    "    \n",
    "    #Read and fill missing values\n",
    "    x = pd.read_csv(file_name, low_memory=False,parse_dates=[\"datetime\"],index_col=0)\n",
    "    x[\"datetime\"]=pd.to_datetime(x[\"datetime\"])\n",
    "    x=x.set_index('datetime')\n",
    "    x.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "    x=x.interpolate(method='ffill')\n",
    "\n",
    "    ## Get variance metadata ####\n",
    "    var=x.iloc[:,:3].var()\n",
    "    for i in range(3):\n",
    "        x.loc[:, 'var'+str(i)] = var.iloc[i]\n",
    "    ### end get variance ####\n",
    "\n",
    "    ## Get sem metadata ####\n",
    "    sem=x.iloc[:,:3].sem()\n",
    "    for i in range(3):\n",
    "        x.loc[:, 'sem'+str(i)] = sem.iloc[i]\n",
    "    ### end get sem ####\n",
    "\n",
    "\n",
    "    ## Get min-max capacity physical metadata ####\n",
    "    mini=x.iloc[:,3:5].min()\n",
    "    maxi=x.iloc[:,3:5].max()\n",
    "    for i in range(2):\n",
    "        x.loc[:, 'min'+str(i)] = mini.iloc[i]\n",
    "    for i in range(2):\n",
    "        x.loc[:, 'max'+str(i)] = maxi.iloc[i]    \n",
    "    ### end get sem ####    \n",
    "    \n",
    "    combined_data.append(x)\n",
    "    sn=os.path.basename(file_name)\n",
    "    station_name.append(sn[:len(sn)-4])\n",
    "    x.to_csv(\"prep_data/\"+sn, index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d045c67",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openstef.model.regressors.dazls import Dazls\n",
    "\n",
    "\n",
    "combined_data = []\n",
    "station_name = []\n",
    "\n",
    "\n",
    "# Read prepared data\n",
    "for file_name in glob.glob('prep_data/*.csv'):\n",
    "    x = pd.read_csv(file_name, low_memory=False, parse_dates=[\"datetime\"])\n",
    "    x[\"datetime\"] = pd.to_datetime(x[\"datetime\"])\n",
    "    x = x.set_index('datetime')\n",
    "    x.columns=[x.lower() for x in x.columns]\n",
    "    combined_data.append(x)\n",
    "    sn = os.path.basename(file_name)\n",
    "    station_name.append(sn[:len(sn) - 4])\n",
    "\n",
    "\n",
    "# Split data in train and test (the first substation is being used for the testing)\n",
    "training_data = pd.concat(combined_data[1:])\n",
    "test_data = combined_data[0]\n",
    "target_columns =['total_solar_part', 'total_wind_part']\n",
    "feature_columns = [x for x in test_data.columns if x not in target_columns]\n",
    "print('Testing station:',station_name[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fca6b90",
   "metadata": {},
   "source": [
    "##### Initialize DAZLS model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "375a8b47",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Dazls()\n",
    "# Fit model\n",
    "model.fit(training_data.loc[:,feature_columns], training_data.loc[:,target_columns])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75e033fc",
   "metadata": {},
   "source": [
    "##### Generate a prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66e7dcdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get predicted y\n",
    "y = model.predict(test_data.loc[:,feature_columns])\n",
    "# print prediction performance\n",
    "model.score(test_data.loc[:,target_columns], y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d53da56c",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = test_data.loc[:,target_columns].copy()\n",
    "result['wind_split'] = y[:,0]\n",
    "result['solar_split'] = y[:,1]\n",
    "result.iloc[60:]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba34fcc9",
   "metadata": {},
   "source": [
    "##### Load and store the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ac275ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = 'dazls_stored.sav'\n",
    "joblib.dump(model, filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34f806d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import openstef\n",
    "openstef.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ea3a8c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn\n",
    "sklearn.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d63cdca5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

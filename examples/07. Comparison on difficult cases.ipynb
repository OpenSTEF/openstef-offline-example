{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bfb2e5f7-c103-4df7-ab35-beaf55859d36",
   "metadata": {},
   "source": [
    "# Comparison on difficult cases\n",
    "\n",
    "With this notebook, you can compare how changes to OpenSTEF improve it's performance on concrete cases where the performance of OpenSTEF was not that great. \n",
    "\n",
    "At Alliander, the operations team identified a number of cases where improvements of the forecast accuracy would be very valuable.\n",
    "These cases are stored as 'fixed' train/test data experiments that can be used to systematically measure improvement ideas to OpenSTEF.\n",
    "\n",
    "This notebook uses the `train_pipeline_common` function from OpenSTEF, which allows detailed modifications of the input data.\n",
    "\n",
    "The notebook is structured as follows:\n",
    "- Generate benchmark (needed once)\n",
    "  - For each experiment:\n",
    "    - read data\n",
    "    - train model\n",
    "    - generate forecast\n",
    "  - Calculate performance metrics\n",
    "  - (Optionally: repeat N times to make outcome more robust)\n",
    "- Test improvement idea (import local OpenSTEF)\n",
    "  - For each experiment:\n",
    "    - read data\n",
    "    - train model\n",
    "    - generate forecast\n",
    "  - Calculate performance metrics\n",
    "  - (Optionally: repeat N times to make outcome more robust)\n",
    "- Compare benchmark and improvement idea"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f11a388c-2616-4746-9cd8-fca36aa70af8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import timedelta\n",
    "from tqdm.notebook import tqdm\n",
    "import plotly.express as px\n",
    "from typing import Dict, List\n",
    "\n",
    "# Set plotly as the default pandas plotting backend\n",
    "pd.options.plotting.backend = \"plotly\"\n",
    "import plotly.io as pio\n",
    "\n",
    "pio.renderers.default = \"plotly_mimetype+notebook\"\n",
    "\n",
    "# Import required stuff from OpenSTEF\n",
    "from openstef.data_classes.prediction_job import PredictionJobDataClass\n",
    "from openstef.data_classes.model_specifications import ModelSpecificationDataClass\n",
    "\n",
    "from openstef.metrics.figure import plot_feature_importance\n",
    "from openstef.pipeline.train_model import train_model_pipeline\n",
    "from openstef.pipeline.create_forecast import create_forecast_pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ea90778-982e-40fe-a40e-915e3609027c",
   "metadata": {},
   "source": [
    "## Generate Benchmark\n",
    "Generate a benchmark using the latest OpenSTEF. Only needed once."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c5c864e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load experiment\n",
    "\n",
    "\n",
    "def get_experiments(difficult_location_number: int = 1):\n",
    "    \"\"\"\n",
    "    Returns a list of dicts, with each dict being an experiment with:\n",
    "    - name\n",
    "    - dataset\n",
    "    - test_length # length counted from the end of the data that will be considered the Test period\n",
    "    -\"\"\"\n",
    "    complete_data = pd.read_csv(\n",
    "        f\"data/model_input_difficult_location_{difficult_location_number}.csv\",\n",
    "        parse_dates=True,\n",
    "        index_col=0,\n",
    "    )\n",
    "\n",
    "    # Generate the experiments where we forecast for 1 day each time\n",
    "    # Hardcoded on 10 days for now\n",
    "    # Dataset contains both the 120 days of training data + the 1 day of test data\n",
    "    result = [\n",
    "        dict(  # Predict day 1\n",
    "            name=f\"N1_loc{difficult_location_number}\",\n",
    "            dataset=complete_data.iloc[0 * 24 * 4 : 120 * 24 * 4 + 24 * 4],\n",
    "            test_length=24 * 4,\n",
    "        ),\n",
    "        dict(  # Predict day 2\n",
    "            name=f\"N2_loc{difficult_location_number}\",\n",
    "            dataset=complete_data.iloc[1 * 24 * 4 : 121 * 24 * 4 + 24 * 4],\n",
    "            test_length=24 * 4,\n",
    "        ),\n",
    "        dict(  # Predict day 3\n",
    "            name=f\"N3_loc{difficult_location_number}\",\n",
    "            dataset=complete_data.iloc[2 * 24 * 4 : 122 * 24 * 4 + 24 * 4],\n",
    "            test_length=24 * 4,\n",
    "        ),\n",
    "        dict(  # Predict day 4\n",
    "            name=f\"N4_loc{difficult_location_number}\",\n",
    "            dataset=complete_data.iloc[3 * 24 * 4 : 123 * 24 * 4 + 24 * 4],\n",
    "            test_length=24 * 4,\n",
    "        ),\n",
    "        dict(  # Predict day 5\n",
    "            name=f\"N5_loc{difficult_location_number}\",\n",
    "            dataset=complete_data.iloc[4 * 24 * 4 : 124 * 24 * 4 + 24 * 4],\n",
    "            test_length=24 * 4,\n",
    "        ),\n",
    "        dict(  # Predict day 6\n",
    "            name=f\"N6_loc{difficult_location_number}\",\n",
    "            dataset=complete_data.iloc[5 * 24 * 4 : 125 * 24 * 4 + 24 * 4],\n",
    "            test_length=24 * 4,\n",
    "        ),\n",
    "        dict(  # Predict day 7\n",
    "            name=f\"N7_loc{difficult_location_number}\",\n",
    "            dataset=complete_data.iloc[6 * 24 * 4 : 126 * 24 * 4 + 24 * 4],\n",
    "            test_length=24 * 4,\n",
    "        ),\n",
    "        dict(  # Predict day 8\n",
    "            name=f\"N8_loc{difficult_location_number}\",\n",
    "            dataset=complete_data.iloc[7 * 24 * 4 : 127 * 24 * 4 + 24 * 4],\n",
    "            test_length=24 * 4,\n",
    "        ),\n",
    "        dict(  # Predict day 9\n",
    "            name=f\"N9_loc{difficult_location_number}\",\n",
    "            dataset=complete_data.iloc[8 * 24 * 4 : 128 * 24 * 4 + 24 * 4],\n",
    "            test_length=24 * 4,\n",
    "        ),\n",
    "        dict(  # Predict day 10\n",
    "            name=f\"N10_loc{difficult_location_number}\",\n",
    "            dataset=complete_data.iloc[9 * 24 * 4 : 129 * 24 * 4 + 24 * 4],\n",
    "            test_length=24 * 4,\n",
    "        ),\n",
    "    ]\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0ee9ea9",
   "metadata": {},
   "outputs": [],
   "source": [
    "difficult_location_number = 1\n",
    "experiments = get_experiments(difficult_location_number)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cec0b86b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shared prediction job specs\n",
    "pj = dict(\n",
    "    id=1,  # Should be updated for each experiment\n",
    "    model=\"xgb\",\n",
    "    quantiles=[0.05, 0.1, 0.3, 0.7, 0.9, 0.95],\n",
    "    name=\"benchmark\",\n",
    ")\n",
    "\n",
    "\n",
    "# These are not relevant...\n",
    "pj_specs_that_should_be_optional_in_future_openstef_versions = dict(\n",
    "    forecast_type=\"demand\",\n",
    "    lat=52.0,\n",
    "    lon=5.0,\n",
    "    horizon_minutes=47 * 60,\n",
    "    description=\"description\",\n",
    "    resolution_minutes=15,\n",
    "    hyper_params={},\n",
    "    feature_names=None,\n",
    ")\n",
    "\n",
    "pj.update(pj_specs_that_should_be_optional_in_future_openstef_versions)\n",
    "\n",
    "pj = PredictionJobDataClass(**pj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "forecasts = pd.DataFrame()\n",
    "\n",
    "for i, experiment in tqdm(enumerate(experiments)):\n",
    "    print(f\"Starting experiment: {experiment['name']}\")\n",
    "    #############\n",
    "    # Data prep\n",
    "    # Update pj\n",
    "    pj['id'] = i\n",
    "    pj['description'] = experiment['name']\n",
    "    \n",
    "    # Split train and test\n",
    "    train = experiment['dataset'].iloc[:-experiment['test_length']]\n",
    "    realised = experiment['dataset'].iloc[-experiment['test_length']:]['load']\n",
    "    \n",
    "    test = experiment['dataset'].copy(deep=True)\n",
    "    # Set the test data to NaN for the last test_length rows\n",
    "    test.iloc[-experiment['test_length']:,0] = np.nan # This assumes the load column is the first one!\n",
    "    # For forecasting we use the last 14 days of data as historical data, which is used for the lagged features\n",
    "    # So we select the last 14 days of the training data + the day to forecast\n",
    "    test = test.iloc[-15*24*4:]\n",
    "    \n",
    "    ##############\n",
    "    # train model\n",
    "    models = train_model_pipeline(\n",
    "    pj,\n",
    "    train,\n",
    "    check_old_model_age=False,\n",
    "    mlflow_tracking_uri=\"./mlflow_trained_models\",\n",
    "    artifact_folder=\"./mlflow_artifacts\",\n",
    "    )\n",
    "    \n",
    "    #################\n",
    "    # Generate forecast\n",
    "    forecast = create_forecast_pipeline(pj, test, mlflow_tracking_uri=\"./mlflow_trained_models\",)\n",
    "    # Add realised to forecast\n",
    "    forecast['load'] = realised\n",
    "    # Only keep region that was actually forecasted\n",
    "    forecast = forecast.iloc[-experiment['test_length']:]\n",
    "    \n",
    "    #######\n",
    "    # Store forecast / Concatenate results\n",
    "    forecasts = pd.concat([forecasts, forecast], axis=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1086ba1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get all column names that start with \"quantile\"\n",
    "quantile_columns = [col for col in forecasts.columns if col.startswith(\"quantile\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d188e25a",
   "metadata": {},
   "outputs": [],
   "source": [
    "lambda x: 'quantile' in x.name and x.name != quantile_columns[0]\n",
    "# apply lambda function to the columns\n",
    "forecasts[quantile_columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae15a268",
   "metadata": {},
   "outputs": [],
   "source": [
    "##########\n",
    "# Plot results\n",
    "fig = forecasts[['load', 'forecast'] + quantile_columns].plot()\n",
    "fig.update_traces(\n",
    "        line=dict(color=\"red\", width=2),\n",
    "        selector=lambda x: 'load' in x.name)\n",
    "fig.update_traces(\n",
    "        line=dict(color=\"blue\", width=2),\n",
    "        selector=lambda x: 'forecast' in x.name)\n",
    "\n",
    "# Show a green area between all quantile_columns\n",
    "fig.update_traces(\n",
    "        fill=\"tonexty\",\n",
    "        fillcolor=\"rgba(0,255,0,0.2)\",\n",
    "        selector=lambda x: 'quantile' in x.name)\n",
    "\n",
    "# Set all quantile traces to be green with half transparency\n",
    "fig.update_traces(\n",
    "        line=dict(color=\"green\", width=1),\n",
    "        opacity=0.5,\n",
    "        selector=lambda x: 'quantile' in x.name)\n",
    "\n",
    "fig.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c7d66cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "############\n",
    "# Calculate metrics\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(forecasts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

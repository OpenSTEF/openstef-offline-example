{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bfb2e5f7-c103-4df7-ab35-beaf55859d36",
   "metadata": {},
   "source": [
    "# Comparison on difficult cases\n",
    "\n",
    "With this notebook, you can compare how changes to OpenSTEF improve it's performance on concrete cases where the performance of OpenSTEF was not that great. \n",
    "\n",
    "At Alliander, the operations team identified a number of cases where improvements of the forecast accuracy would be very valuable.\n",
    "These cases are stored as 'fixed' train/test data experiments that can be used to systematically measure improvement ideas to OpenSTEF.\n",
    "\n",
    "This notebook uses the `train_pipeline_common` function from OpenSTEF, which allows detailed modifications of the input data.\n",
    "\n",
    "The notebook is structured as follows:\n",
    "- Generate benchmark (needed once)\n",
    "  - For each experiment:\n",
    "    - read data\n",
    "    - train model\n",
    "    - generate forecast\n",
    "  - Calculate performance metrics\n",
    "  - (Optionally: repeat N times to make outcome more robust)\n",
    "- Test improvement idea (import local OpenSTEF)\n",
    "  - For each experiment:\n",
    "    - read data\n",
    "    - train model\n",
    "    - generate forecast\n",
    "  - Calculate performance metrics\n",
    "  - (Optionally: repeat N times to make outcome more robust)\n",
    "- Compare benchmark and improvement idea"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f11a388c-2616-4746-9cd8-fca36aa70af8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import timedelta\n",
    "from tqdm.notebook import tqdm\n",
    "import plotly.express as px\n",
    "from typing import Dict, List\n",
    "from sklearn import metrics\n",
    "import plotly.subplots as subplots\n",
    "import scipy.signal as signal\n",
    "from sklearn import metrics\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "# Set plotly as the default pandas plotting backend\n",
    "pd.options.plotting.backend = \"plotly\"\n",
    "import plotly.io as pio\n",
    "\n",
    "pio.renderers.default = \"plotly_mimetype+notebook\"\n",
    "\n",
    "# Import required stuff from OpenSTEF\n",
    "from openstef.data_classes.prediction_job import PredictionJobDataClass\n",
    "from openstef.data_classes.model_specifications import ModelSpecificationDataClass\n",
    "\n",
    "from openstef.metrics.figure import plot_feature_importance\n",
    "from openstef.pipeline.train_model import train_model_pipeline\n",
    "from openstef.pipeline.create_forecast import create_forecast_pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfc9341f",
   "metadata": {},
   "source": [
    "## Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7799b12",
   "metadata": {},
   "outputs": [],
   "source": [
    "difficult_location_number = 1\n",
    "quantiles = [0.05, 0.1, 0.3, 0.5, 0.7, 0.9, 0.95]\n",
    "\n",
    "PLOT_POSITIVE_PEAKS = True\n",
    "PLOT_NEGATIVE_PEAKS = False\n",
    "compare_positive_peaks = True  # For quantile peak detection evaluation\n",
    "\n",
    "\n",
    "PLOT_ROLLING_WINDOW = False\n",
    "\n",
    "percentile = 50  # Select forecast percentile to evaluate\n",
    "\n",
    "# Less important settings:\n",
    "\n",
    "# For rolling window calculations. 1 sample = window of 15 minutes\n",
    "window_size = 4 * 24\n",
    "\n",
    "# Peak detection settings\n",
    "peak_evaluation_width = 10\n",
    "peak_miss_threshold = 1e6\n",
    "load_threshold_min_quantile = 0.1\n",
    "load_threshold_max_quantile = 0.9\n",
    "peak_distance = 4 * 3  # 3 hours"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ea90778-982e-40fe-a40e-915e3609027c",
   "metadata": {},
   "source": [
    "## Generate Benchmark\n",
    "Generate a benchmark using the latest OpenSTEF. Only needed once."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c5c864e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load experiment\n",
    "def get_experiments(difficult_location_number: int = 1):\n",
    "    \"\"\"\n",
    "    Returns a list of dicts, with each dict being an experiment with:\n",
    "    - name\n",
    "    - dataset\n",
    "    - test_length # length counted from the end of the data that will be considered the Test period\n",
    "    -\"\"\"\n",
    "    complete_data = pd.read_csv(\n",
    "        f\"data/model_input_difficult_location_{difficult_location_number}.csv\",\n",
    "        parse_dates=True,\n",
    "        index_col=0,\n",
    "    )\n",
    "\n",
    "    # Generate the experiments where we forecast for 1 day each time\n",
    "    # Hardcoded on 10 days for now\n",
    "    # Dataset contains both the 120 days of training data + the 1 day of test data\n",
    "    result = [\n",
    "        dict(  # Predict day 1\n",
    "            name=f\"N1_loc{difficult_location_number}\",\n",
    "            dataset=complete_data.iloc[0 * 24 * 4 : 120 * 24 * 4 + 24 * 4],\n",
    "            test_length=24 * 4,\n",
    "        ),\n",
    "        dict(  # Predict day 2\n",
    "            name=f\"N2_loc{difficult_location_number}\",\n",
    "            dataset=complete_data.iloc[1 * 24 * 4 : 121 * 24 * 4 + 24 * 4],\n",
    "            test_length=24 * 4,\n",
    "        ),\n",
    "        dict(  # Predict day 3\n",
    "            name=f\"N3_loc{difficult_location_number}\",\n",
    "            dataset=complete_data.iloc[2 * 24 * 4 : 122 * 24 * 4 + 24 * 4],\n",
    "            test_length=24 * 4,\n",
    "        ),\n",
    "        dict(  # Predict day 4\n",
    "            name=f\"N4_loc{difficult_location_number}\",\n",
    "            dataset=complete_data.iloc[3 * 24 * 4 : 123 * 24 * 4 + 24 * 4],\n",
    "            test_length=24 * 4,\n",
    "        ),\n",
    "        dict(  # Predict day 5\n",
    "            name=f\"N5_loc{difficult_location_number}\",\n",
    "            dataset=complete_data.iloc[4 * 24 * 4 : 124 * 24 * 4 + 24 * 4],\n",
    "            test_length=24 * 4,\n",
    "        ),\n",
    "        dict(  # Predict day 6\n",
    "            name=f\"N6_loc{difficult_location_number}\",\n",
    "            dataset=complete_data.iloc[5 * 24 * 4 : 125 * 24 * 4 + 24 * 4],\n",
    "            test_length=24 * 4,\n",
    "        ),\n",
    "        dict(  # Predict day 7\n",
    "            name=f\"N7_loc{difficult_location_number}\",\n",
    "            dataset=complete_data.iloc[6 * 24 * 4 : 126 * 24 * 4 + 24 * 4],\n",
    "            test_length=24 * 4,\n",
    "        ),\n",
    "        dict(  # Predict day 8\n",
    "            name=f\"N8_loc{difficult_location_number}\",\n",
    "            dataset=complete_data.iloc[7 * 24 * 4 : 127 * 24 * 4 + 24 * 4],\n",
    "            test_length=24 * 4,\n",
    "        ),\n",
    "        dict(  # Predict day 9\n",
    "            name=f\"N9_loc{difficult_location_number}\",\n",
    "            dataset=complete_data.iloc[8 * 24 * 4 : 128 * 24 * 4 + 24 * 4],\n",
    "            test_length=24 * 4,\n",
    "        ),\n",
    "        dict(  # Predict day 10\n",
    "            name=f\"N10_loc{difficult_location_number}\",\n",
    "            dataset=complete_data.iloc[9 * 24 * 4 : 129 * 24 * 4 + 24 * 4],\n",
    "            test_length=24 * 4,\n",
    "        ),\n",
    "    ]\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0ee9ea9",
   "metadata": {},
   "outputs": [],
   "source": [
    "experiments = get_experiments(difficult_location_number)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cec0b86b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shared prediction job specs\n",
    "pj = dict(\n",
    "    id=1,  # Should be updated for each experiment\n",
    "    model=\"xgb\",\n",
    "    quantiles=quantiles,\n",
    "    name=\"benchmark\",\n",
    ")\n",
    "\n",
    "\n",
    "# These are not relevant...\n",
    "pj_specs_that_should_be_optional_in_future_openstef_versions = dict(\n",
    "    forecast_type=\"demand\",\n",
    "    lat=52.0,\n",
    "    lon=5.0,\n",
    "    horizon_minutes=47 * 60,\n",
    "    description=\"description\",\n",
    "    resolution_minutes=15,\n",
    "    hyper_params={},\n",
    "    feature_names=None,\n",
    ")\n",
    "\n",
    "pj.update(pj_specs_that_should_be_optional_in_future_openstef_versions)\n",
    "\n",
    "pj = PredictionJobDataClass(**pj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "forecasts = pd.DataFrame()\n",
    "\n",
    "for i, experiment in tqdm(enumerate(experiments)):\n",
    "    print(f\"Starting experiment: {experiment['name']}\")\n",
    "    #############\n",
    "    # Data prep\n",
    "    # Update pj\n",
    "    pj[\"id\"] = i\n",
    "    pj[\"description\"] = experiment[\"name\"]\n",
    "\n",
    "    # Split train and test\n",
    "    train = experiment[\"dataset\"].iloc[: -experiment[\"test_length\"]]\n",
    "    realised = experiment[\"dataset\"].iloc[-experiment[\"test_length\"] :][\"load\"]\n",
    "\n",
    "    test = experiment[\"dataset\"].copy(deep=True)\n",
    "    # Set the test data to NaN for the last test_length rows\n",
    "    test.iloc[-experiment[\"test_length\"] :, 0] = (\n",
    "        np.nan\n",
    "    )  # This assumes the load column is the first one!\n",
    "    # For forecasting we use the last 14 days of data as historical data, which is used for the lagged features\n",
    "    # So we select the last 14 days of the training data + the day to forecast\n",
    "    test = test.iloc[-15 * 24 * 4 :]\n",
    "\n",
    "    ##############\n",
    "    # train model\n",
    "    models = train_model_pipeline(\n",
    "        pj,\n",
    "        train,\n",
    "        check_old_model_age=False,\n",
    "        mlflow_tracking_uri=\"./mlflow_trained_models\",\n",
    "        artifact_folder=\"./mlflow_artifacts\",\n",
    "    )\n",
    "\n",
    "    #################\n",
    "    # Generate forecast\n",
    "    forecast = create_forecast_pipeline(\n",
    "        pj,\n",
    "        test,\n",
    "        mlflow_tracking_uri=\"./mlflow_trained_models\",\n",
    "    )\n",
    "    # Add realised to forecast\n",
    "    forecast[\"load\"] = realised\n",
    "    # Only keep region that was actually forecasted\n",
    "    forecast = forecast.iloc[-experiment[\"test_length\"] :]\n",
    "\n",
    "    #######\n",
    "    # Store forecast / Concatenate results\n",
    "    forecasts = pd.concat([forecasts, forecast], axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d6addd9",
   "metadata": {},
   "source": [
    "## Evaluate results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1086ba1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get all column names that start with \"quantile\"\n",
    "quantile_columns = [col for col in forecasts.columns if col.startswith(\"quantile\")]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dda1988a",
   "metadata": {},
   "source": [
    "### Plot forecast against measurements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae15a268",
   "metadata": {},
   "outputs": [],
   "source": [
    "##########\n",
    "# Plot results\n",
    "fig = forecasts[[\"load\", \"forecast\"] + quantile_columns].plot()\n",
    "fig.update_traces(line=dict(color=\"red\", width=2), selector=lambda x: \"load\" in x.name)\n",
    "fig.update_traces(\n",
    "    line=dict(color=\"blue\", width=2), selector=lambda x: \"forecast\" in x.name\n",
    ")\n",
    "\n",
    "# Show a green area between all quantile_columns\n",
    "fig.update_traces(\n",
    "    fill=\"tonexty\",\n",
    "    fillcolor=\"rgba(0,255,0,0.2)\",\n",
    "    selector=lambda x: \"quantile\" in x.name,\n",
    ")\n",
    "\n",
    "# Set all quantile traces to be green with half transparency\n",
    "fig.update_traces(\n",
    "    line=dict(color=\"green\", width=1),\n",
    "    opacity=0.5,\n",
    "    selector=lambda x: \"quantile\" in x.name,\n",
    ")\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rmse_for_window(forecast_window, measurements, window_size):\n",
    "    measurements_window = measurements.loc[forecast_window.index]\n",
    "    mse = metrics.mean_squared_error(measurements_window, forecast_window)\n",
    "    return np.sqrt(mse)\n",
    "\n",
    "\n",
    "def mae_for_window(forecast_window, measurements, window_size):\n",
    "    measurements_window = measurements.loc[forecast_window.index]\n",
    "    return metrics.mean_absolute_error(measurements_window, forecast_window)\n",
    "\n",
    "\n",
    "forecast = forecasts[f\"quantile_P{percentile}\"]\n",
    "measurements = forecasts[\"load\"]\n",
    "rmse = forecast.rolling(window=window_size).apply(\n",
    "    rmse_for_window, args=(measurements, window_size)\n",
    ")\n",
    "mae = forecast.rolling(window=window_size).apply(\n",
    "    mae_for_window, args=(measurements, window_size)\n",
    ")\n",
    "\n",
    "error_df = pd.concat([rmse, mae], axis=1)\n",
    "# Rename columns to \"rmse\" and \"mae\"\n",
    "error_df.columns = [\"rmse\", \"mae\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41d66ef7",
   "metadata": {},
   "source": [
    "### rMAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "593c20aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate rmae between forecast and measurements\n",
    "rmae = np.sqrt(metrics.mean_squared_error(measurements, forecast))\n",
    "print(f\"rMAE: {rmae}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if PLOT_ROLLING_WINDOW:\n",
    "    error_df[[\"rmse\", \"mae\"]].plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "809e7e2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_peaks_from_measurements(\n",
    "    measurements,\n",
    "    thresholds_pos: [int],\n",
    "    thresholds_neg: [int],\n",
    "    positive_peaks: bool = True,\n",
    "    peak_distance: int = 12,\n",
    ") -> [int]:\n",
    "    \"\"\"\n",
    "\n",
    "    Args:\n",
    "        measurements (_type_): _description_\n",
    "        thresholds_pos (int]): _description_\n",
    "        thresholds_neg (int]): _description_\n",
    "        positive_peaks (bool, optional): _description_. Defaults to True.\n",
    "        peak_distance (int, optional): Sample distance between peaks. Defaults to 12 (3 * 4 samples = 3 hours).\n",
    "\n",
    "    Returns:\n",
    "        _type_: _description_\n",
    "    \"\"\"\n",
    "\n",
    "    # measurements = measurements.droplevel([1], axis=1)\n",
    "\n",
    "    # Define constants\n",
    "\n",
    "    peak_distance = 4 * 3  # samples are 15min, 3 hour distance\n",
    "\n",
    "    # Detect peaks\n",
    "    if positive_peaks:\n",
    "        peaks = [\n",
    "            (\n",
    "                \"terminal_id\",\n",
    "                signal.find_peaks(\n",
    "                    measurements, height=thresholds_pos, distance=peak_distance\n",
    "                ),\n",
    "            )\n",
    "        ]\n",
    "    else:\n",
    "        peaks = [\n",
    "            (\n",
    "                \"terminal_id\",\n",
    "                signal.find_peaks(\n",
    "                    -measurements, height=-thresholds_neg, distance=peak_distance\n",
    "                ),\n",
    "            )\n",
    "        ]\n",
    "\n",
    "    return peaks\n",
    "\n",
    "\n",
    "def is_missed_peak_in_window(\n",
    "    peak_measurement_window, peak_forecast_window, miss_threshold: int\n",
    "):\n",
    "    mape = metrics.mean_absolute_error(peak_measurement_window, peak_forecast_window)\n",
    "    return mape > miss_threshold\n",
    "\n",
    "\n",
    "def is_missed_peak(\n",
    "    comp_df, peak_index: int, peak_evaluation_width: int, mape_threshold: int\n",
    "):\n",
    "    min_index = max(peak_index - peak_evaluation_width, 0)\n",
    "    max_index = min(peak_index + peak_evaluation_width, len(comp_df))\n",
    "    measurement_window = comp_df.loc[:, \"Measured\"][min_index:max_index]\n",
    "    forecast_window = comp_df.loc[:, \"Forecast\"][measurement_window.index]\n",
    "    return is_missed_peak_in_window(measurement_window, forecast_window, mape_threshold)\n",
    "\n",
    "\n",
    "def get_missed_peaks(\n",
    "    comp_df, peak_index_list: [int], peak_evaluation_width: int, mape_threshold: int\n",
    "):\n",
    "    return [\n",
    "        peak\n",
    "        for peak in peak_index_list\n",
    "        if is_missed_peak(comp_df, peak, peak_evaluation_width, mape_threshold)\n",
    "    ]\n",
    "\n",
    "\n",
    "def plot_missed_peaks(\n",
    "    comp_df,\n",
    "    peak_index_list: [int],\n",
    "    peak_evaluation_width: int,\n",
    "    mape_threshold: int,\n",
    "    subplots,\n",
    "    plot_index,\n",
    "):\n",
    "\n",
    "    line_graph = px.line(comp_df)\n",
    "    for trace in range(len(line_graph[\"data\"])):\n",
    "        subplots.append_trace(line_graph[\"data\"][trace], row=plot_index, col=1)\n",
    "\n",
    "    total_missed = 0\n",
    "    for peak in peak_index_list:\n",
    "        min_index = max(peak - peak_evaluation_width, 0)\n",
    "        max_index = min(peak + peak_evaluation_width, len(comp_df))\n",
    "        measurement_window = comp_df.loc[:, \"Measured\"][min_index:max_index]\n",
    "        forecast_window = comp_df.loc[:, \"Forecast\"][measurement_window.index]\n",
    "\n",
    "        start_rectangle = measurement_window.index[0]\n",
    "        end_rectangle = measurement_window.index[-1]\n",
    "\n",
    "        # Plot red area around missed peak\n",
    "        if is_missed_peak_in_window(\n",
    "            measurement_window, forecast_window, mape_threshold\n",
    "        ):\n",
    "            total_missed += 1\n",
    "            subplots.add_vrect(\n",
    "                x0=start_rectangle,\n",
    "                x1=end_rectangle,\n",
    "                line_width=0,\n",
    "                fillcolor=\"red\",\n",
    "                opacity=0.2,\n",
    "                row=plot_index,\n",
    "                col=1,\n",
    "            )\n",
    "        else:\n",
    "            subplots.add_vrect(\n",
    "                x0=start_rectangle,\n",
    "                x1=end_rectangle,\n",
    "                line_width=0,\n",
    "                fillcolor=\"blue\",\n",
    "                opacity=0.1,\n",
    "                row=plot_index,\n",
    "                col=1,\n",
    "            )\n",
    "\n",
    "    return total_missed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d3996b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "forecast_percentile = percentile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b99cc94",
   "metadata": {},
   "outputs": [],
   "source": [
    "thresholds_pos = measurements.quantile(load_threshold_max_quantile)\n",
    "thresholds_neg = measurements.quantile(load_threshold_min_quantile)\n",
    "\n",
    "peaks_pos = get_peaks_from_measurements(\n",
    "    measurements,\n",
    "    thresholds_pos=thresholds_pos,\n",
    "    thresholds_neg=thresholds_neg,\n",
    "    positive_peaks=True,\n",
    "    peak_distance=peak_distance,\n",
    ")\n",
    "peaks_neg = get_peaks_from_measurements(\n",
    "    measurements,\n",
    "    thresholds_pos=thresholds_pos,\n",
    "    thresholds_neg=thresholds_neg,\n",
    "    positive_peaks=False,\n",
    "    peak_distance=peak_distance,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c731002a",
   "metadata": {},
   "outputs": [],
   "source": [
    "if PLOT_POSITIVE_PEAKS:\n",
    "    fig = subplots.make_subplots(rows=1, cols=1)\n",
    "\n",
    "    for i, peak_list in enumerate(peaks_pos):\n",
    "        terminal_id, peaks = peak_list\n",
    "        measurements_for_terminal = measurements.rename(\"Measured\")\n",
    "        forecasts_for_terminal = forecasts[f\"quantile_P{forecast_percentile}\"].rename(\n",
    "            \"Forecast\"\n",
    "        )\n",
    "\n",
    "        comp_df = pd.concat([measurements_for_terminal, forecasts_for_terminal], axis=1)\n",
    "        print(comp_df.isnull().values.any())\n",
    "        total_missed_pos = plot_missed_peaks(\n",
    "            comp_df, peaks[0], peak_evaluation_width, peak_miss_threshold, fig, i + 1\n",
    "        )\n",
    "\n",
    "        print(f\"Missed positive peaks: {total_missed_pos}\")\n",
    "\n",
    "    fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c10b07c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "if PLOT_NEGATIVE_PEAKS:\n",
    "    fig = subplots.make_subplots(rows=1, cols=1)\n",
    "\n",
    "    for i, peak_list in enumerate(peaks_neg):\n",
    "        terminal_id, peaks = peak_list\n",
    "        measurements_for_terminal = measurements.rename(\"Measured\")\n",
    "        forecasts_for_terminal = forecasts[f\"quantile_P{forecast_percentile}\"].rename(\n",
    "            \"Forecast\"\n",
    "        )\n",
    "\n",
    "        comp_df = pd.concat([measurements_for_terminal, forecasts_for_terminal], axis=1)\n",
    "        print(comp_df.isnull().values.any())\n",
    "        total_missed_pos = plot_missed_peaks(\n",
    "            comp_df, peaks[0], peak_evaluation_width, peak_miss_threshold, fig, i + 1\n",
    "        )\n",
    "\n",
    "        print(f\"Missed negative peaks: {total_missed_pos}\")\n",
    "\n",
    "    fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6169dd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "if compare_positive_peaks:\n",
    "    detected_peaks = peaks_pos\n",
    "else:\n",
    "    detected_peaks = peaks_neg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6c460f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "forecasts_all_percentiles = forecasts[quantile_columns]\n",
    "terminals = [\"terminal_id\"]\n",
    "percentiles = quantile_columns\n",
    "\n",
    "missed_peaks_list = {\n",
    "    terminal: {percentile: np.NaN for percentile in percentiles}\n",
    "    for terminal in terminals\n",
    "}\n",
    "\n",
    "for percentile in percentiles:\n",
    "    forecast_single_percentile = forecasts_all_percentiles[percentile]\n",
    "\n",
    "    for i, peak_list in enumerate(detected_peaks):\n",
    "        terminal_id, peaks = peak_list\n",
    "        measurements_for_terminal = measurements.rename(\"Measured\")\n",
    "        forecasts_for_terminal = forecast_single_percentile.rename(\"Forecast\")\n",
    "\n",
    "        comp_df = pd.concat([measurements_for_terminal, forecasts_for_terminal], axis=1)\n",
    "        missed_peaks_single_percentile_term = get_missed_peaks(\n",
    "            comp_df, peaks[0], peak_evaluation_width, peak_miss_threshold\n",
    "        )\n",
    "        missed_peaks_list[terminal_id][percentile] = len(\n",
    "            missed_peaks_single_percentile_term\n",
    "        )\n",
    "\n",
    "missed_peaks_df = pd.DataFrame(missed_peaks_list)\n",
    "\n",
    "fig = subplots.make_subplots(\n",
    "    rows=1,\n",
    "    shared_yaxes=\"all\",\n",
    "    x_title=\"Percentile\",\n",
    "    y_title=\"Number of missed peaks\",\n",
    "    subplot_titles=terminals,\n",
    ")\n",
    "for i, terminal in enumerate(terminals):\n",
    "    fig.add_trace(\n",
    "        go.Bar(x=percentiles, y=missed_peaks_df[terminal], name=terminal),\n",
    "        row=i + 1,\n",
    "        col=1,\n",
    "    )\n",
    "\n",
    "fig.update_layout(title_text=\"Missed peaks per terminal and percentile\", height=1000)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eca21390",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
